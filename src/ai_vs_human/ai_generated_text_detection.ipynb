{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Generated Text Detection\n",
    "\n",
    "Stacked ensemble (DeBERTa-v3 + LightGBM + TF-IDF/SGD) on `merged_ai_human_multisocial_features_train.csv` / `merged_ai_human_multisocial_features_test.csv` when present (0=Human, 1=AI). Flow: setup -> data check -> Model A/B/C CV (OOF) -> stacking -> inference. Text column: `text`; label column: `label`; numerical features like `burstiness`, `perplexity_score`, `lexical_diversity`, `gunning_fog_index`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2305080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if running in a fresh environment\n",
    "# !pip install -q pandas numpy torch torchvision torchaudio transformers datasets lightgbm scikit-learn\n",
    "# For CUDA builds of PyTorch: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4dfd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CuTfidfVectorizer\n",
    "from cuml.linear_model import MBSGDClassifier\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "BASE_MODEL = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "N_SPLITS = 5\n",
    "STACK_BASE_MODELS = [\"deberta\", \"lgb\", \"sgd\"]\n",
    "\n",
    "CWD = Path.cwd()\n",
    "data_env = os.getenv(\"DATA_PATH\")\n",
    "candidate_paths = [\n",
    "    Path(data_env).expanduser() if data_env else None,\n",
    "    CWD / \"merged_ai_human_multisocial_features_train.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features_train.csv\",\n",
    "    CWD / \"merged_ai_human_multisocial_features.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features.csv\",\n",
    "    CWD / \"ai_human_content_detection_dataset.csv\",\n",
    "    CWD / \"src/ai_vs_human/ai_human_content_detection_dataset.csv\",\n",
    "]\n",
    "candidate_paths = [p for p in candidate_paths if p is not None]\n",
    "for candidate_path in candidate_paths:\n",
    "    if candidate_path.exists():\n",
    "        DATA_PATH = candidate_path\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"No training data file found. Set DATA_PATH or place merged_ai_human_multisocial_features_train.csv (or merged_ai_human_multisocial_features.csv / ai_human_content_detection_dataset.csv) in the repo.\"\n",
    "    )\n",
    "print(f\"Using training file: {DATA_PATH}\")\n",
    "\n",
    "WORK_DIR = DATA_PATH.parent\n",
    "MODEL_DIR = WORK_DIR / \"models\" / \"deberta_v3_base\"\n",
    "LGB_MODEL_DIR = WORK_DIR / \"models\" / \"lightgbm\"\n",
    "LGB_MODEL_DIR_LEGACY = WORK_DIR / \"models\" / \"lightgbm_numeric\"\n",
    "SGD_MODEL_DIR = WORK_DIR / \"models\" / \"tfidf_sgd\"\n",
    "STACK_MODEL_DIR = WORK_DIR / \"models\" / \"stack_meta\"\n",
    "for path in (MODEL_DIR, LGB_MODEL_DIR, LGB_MODEL_DIR_LEGACY, SGD_MODEL_DIR, STACK_MODEL_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR = WORK_DIR / \"oof\"\n",
    "OOF_DIR.mkdir(exist_ok=True)\n",
    "paired_test = None\n",
    "if DATA_PATH.suffix:\n",
    "    name = DATA_PATH.name\n",
    "    if name.endswith(\"_train\" + DATA_PATH.suffix):\n",
    "        paired_test = DATA_PATH.with_name(name.replace(\"_train\", \"_test\"))\n",
    "    else:\n",
    "        paired_test = DATA_PATH.with_name(DATA_PATH.stem + \"_test\" + DATA_PATH.suffix)\n",
    "test_env = os.getenv(\"TEST_PATH\")\n",
    "test_candidates = [\n",
    "    Path(test_env).expanduser() if test_env else None,\n",
    "    paired_test,\n",
    "    WORK_DIR / \"merged_ai_human_multisocial_features_test.csv\",\n",
    "    WORK_DIR / \"ai_human_content_detection_test.csv\",\n",
    "    WORK_DIR / \"ai_human_content_detection_dataset.csv\",\n",
    "    DATA_PATH,  # fallback: reuse train data so inference still runs\n",
    "]\n",
    "test_candidates = [p for p in test_candidates if p is not None]\n",
    "TEST_PATH = next((p for p in test_candidates if p.exists()), test_candidates[0])\n",
    "print(f\"Using test file: {TEST_PATH}\")\n",
    "if TEST_PATH == DATA_PATH:\n",
    "    print(\"TEST_PATH not provided; using training data as a smoke-test for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3fa7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (928709, 53)\n",
      "Numeric features (40): ['word_count', 'character_count', 'sentence_count', 'lexical_diversity', 'avg_sentence_length', 'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease', 'gunning_fog_index', 'grammar_errors']...\n",
      "                                                text  label\n",
      "0  Cars. Cars have been around since they became ...      0\n",
      "1  Transportation is a large necessity in most co...      0\n",
      "                        count         mean          std  min        25%  \\\n",
      "word_count           928709.0   210.955885   224.147192  0.0  13.000000   \n",
      "character_count      928709.0  1205.169750  1277.843478  1.0  79.000000   \n",
      "sentence_count       928709.0    11.533810    11.357867  1.0   1.000000   \n",
      "lexical_diversity    928709.0     0.685995     0.258201  0.0   0.441894   \n",
      "avg_sentence_length  928709.0    15.167817    11.489294  0.0   8.000000   \n",
      "\n",
      "                            50%     75%      max  \n",
      "word_count           155.000000   368.0   1680.0  \n",
      "character_count      871.000000  2099.0  16094.0  \n",
      "sentence_count         8.000000    20.0    276.0  \n",
      "lexical_diversity      0.633929     1.0      1.0  \n",
      "avg_sentence_length   15.000000    20.0    754.0  \n",
      "Top correlated numeric features with label:\n",
      "model_human            -1.000000\n",
      "lang_unknown           -0.507416\n",
      "ds_ai_human            -0.507416\n",
      "ds_multisocial          0.507416\n",
      "word_count             -0.506573\n",
      "predictability_score   -0.487444\n",
      "lexical_diversity       0.486983\n",
      "sentence_count         -0.466945\n",
      "length                 -0.459179\n",
      "character_count        -0.459179\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data and inspect\n",
    "train_df = pd.read_csv(DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "alt_text_cols = (\"text_content\",)\n",
    "meta_prefixes = (\"src_\",)  # exclude metadata to avoid leakage\n",
    "\n",
    "if text_col not in train_df.columns:\n",
    "    for alt_col in alt_text_cols:\n",
    "        if alt_col in train_df.columns:\n",
    "            train_df = train_df.rename(columns={alt_col: text_col})\n",
    "            break\n",
    "if text_col not in train_df.columns:\n",
    "    raise ValueError(f\"Training data missing `{text_col}` column; set DATA_PATH to a file containing text.\")\n",
    "\n",
    "num_cols = [\n",
    "    c\n",
    "    for c in train_df.columns\n",
    "    if c not in [text_col, label_col]\n",
    "    and pd.api.types.is_numeric_dtype(train_df[c])\n",
    "    and not any(c.startswith(pref) for pref in meta_prefixes)\n",
    "]\n",
    "\n",
    "train_df[num_cols] = train_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "num_medians = train_df[num_cols].median()\n",
    "train_df[num_cols] = train_df[num_cols].fillna(num_medians)\n",
    "if test_df is not None:\n",
    "    if text_col not in test_df.columns:\n",
    "        for alt_col in alt_text_cols:\n",
    "            if alt_col in test_df.columns:\n",
    "                test_df = test_df.rename(columns={alt_col: text_col})\n",
    "                break\n",
    "    if text_col not in test_df.columns:\n",
    "        raise ValueError(f\"Test data missing `{text_col}` column; set TEST_PATH to a file containing text.\")\n",
    "    missing_num_cols = [c for c in num_cols if c not in test_df.columns]\n",
    "    for col in missing_num_cols:\n",
    "        test_df[col] = num_medians[col]\n",
    "    if missing_num_cols:\n",
    "        print(f\"Filled missing numeric columns in test set: {len(missing_num_cols)} (using train medians)\")\n",
    "    test_df[num_cols] = test_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(num_medians)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Numeric features ({len(num_cols)}): {num_cols[:10]}{'...' if len(num_cols) > 10 else ''}\")\n",
    "print(train_df[[text_col, label_col]].head(2))\n",
    "print(train_df[num_cols].describe().T.head())\n",
    "\n",
    "# Quick correlation check to spot potential leakage/high-signal numeric features\n",
    "corr = (\n",
    "    train_df[num_cols + [label_col]]\n",
    "    .corr()[label_col]\n",
    "    .drop(label_col)\n",
    "    .sort_values(key=np.abs, ascending=False)\n",
    ")\n",
    "print(\"Top correlated numeric features with label:\")\n",
    "print(corr.head(10))\n",
    "\n",
    "y = train_df[label_col].astype(int).values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "folds = list(skf.split(train_df[text_col], y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c62fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper classes and metrics\n",
    "class HFTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, text_col, label_col=None, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.loc[idx, self.text_col])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "        if self.label_col is not None:\n",
    "            enc[\"labels\"] = int(self.df.loc[idx, self.label_col])\n",
    "        return enc\n",
    "\n",
    "\n",
    "def softmax_logits(logits):\n",
    "    logits = torch.tensor(logits)\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs[:, 1]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = softmax_logits(logits)\n",
    "    return {\"roc_auc\": roc_auc_score(labels, probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc9f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_592377/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='23218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/23218 00:02 < 8:41:13, 0.74 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     58\u001b[39m         args = TrainingArguments(**fallback_kwargs)\n\u001b[32m     60\u001b[39m trainer = Trainer(\n\u001b[32m     61\u001b[39m     model=model,\n\u001b[32m     62\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     68\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m preds = trainer.predict(val_ds).predictions\n\u001b[32m     73\u001b[39m oof_deberta[val_idx] = softmax_logits(preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:121\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    119\u001b[39m         thread.start()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    123\u001b[39m     _worker(\u001b[32m0\u001b[39m, modules[\u001b[32m0\u001b[39m], inputs[\u001b[32m0\u001b[39m], kwargs_tup[\u001b[32m0\u001b[39m], devices[\u001b[32m0\u001b[39m], streams[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1147\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1167\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1168\u001b[39m         lock.release()\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Model A: DeBERTa-v3-base with Stratified K-Fold OOF\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "oof_deberta = np.zeros(len(train_df))\n",
    "deberta_model_paths = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n[Model A] Fold {fold+1}/{N_SPLITS}\")\n",
    "    fold_dir = MODEL_DIR / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = HFTextDataset(train_df.iloc[train_idx], tokenizer, text_col, label_col, MAX_LENGTH)\n",
    "    val_ds = HFTextDataset(train_df.iloc[val_idx], tokenizer, text_col, label_col, MAX_LENGTH)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, num_labels=2\n",
    "    )\n",
    "\n",
    "    training_kwargs = dict(\n",
    "        output_dir=str(fold_dir),\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Handle TrainingArguments API differences across transformers versions\n",
    "    try:\n",
    "        args = TrainingArguments(\n",
    "            **training_kwargs,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        try:\n",
    "            args = TrainingArguments(\n",
    "                **training_kwargs,\n",
    "                evaluate_during_training=True,\n",
    "                eval_steps=500,\n",
    "                save_steps=500,\n",
    "            )\n",
    "        except TypeError:\n",
    "            fallback_kwargs = training_kwargs.copy()\n",
    "            for key in (\"load_best_model_at_end\", \"metric_for_best_model\", \"greater_is_better\"):\n",
    "                fallback_kwargs.pop(key, None)\n",
    "            args = TrainingArguments(**fallback_kwargs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    preds = trainer.predict(val_ds).predictions\n",
    "    oof_deberta[val_idx] = softmax_logits(preds)\n",
    "\n",
    "    best_dir = trainer.state.best_model_checkpoint or str(fold_dir / \"best\")\n",
    "    if trainer.state.best_model_checkpoint is None:\n",
    "        trainer.save_model(best_dir)\n",
    "    deberta_model_paths.append(best_dir)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "deberta_oof_auc = roc_auc_score(y, oof_deberta)\n",
    "print(f\"Model A OOF ROC-AUC: {deberta_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_deberta\": oof_deberta}).to_csv(OOF_DIR / \"oof_deberta.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: LightGBM on numerical features only (Optuna tuning)\n",
    "import optuna\n",
    "from optuna.integration import lightgbm as lgb_optuna\n",
    "\n",
    "oof_lgb = np.zeros(len(train_df))\n",
    "lgb_models = []\n",
    "lgb_model_paths = []\n",
    "lgb_fold_auc = []\n",
    "\n",
    "# Tuning con los mismos folds que el CV de entrenamiento\n",
    "train_data = lgb.Dataset(train_df[num_cols], label=y)\n",
    "tuner = lgb_optuna.LightGBMTunerCV(\n",
    "    params={\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"verbosity\": -1,\n",
    "        \"feature_pre_filter\": False,\n",
    "        \"scale_pos_weight\": (len(y) - y.sum()) / y.sum(),\n",
    "    },\n",
    "    train_set=train_data,\n",
    "    folds=folds,\n",
    "    num_boost_round=2500,\n",
    "    callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "    return_cvbooster=True,\n",
    "    study=optuna.create_study(direction=\"maximize\"),\n",
    ")\n",
    "# Ejecuta tuning (ajusta time_budget o usa study.sampler si quieres limitar)\n",
    "tuner.run()\n",
    "\n",
    "best_params = tuner.best_params\n",
    "best_cvbooster = None\n",
    "if hasattr(tuner, \"get_best_booster\"):\n",
    "    try:\n",
    "        best_cvbooster = tuner.get_best_booster()\n",
    "    except Exception:\n",
    "        best_cvbooster = None\n",
    "best_iter = getattr(best_cvbooster, \"best_iteration\", None) if best_cvbooster is not None else None\n",
    "if best_iter is None:\n",
    "    best_iter = best_params.get(\"num_boost_round\") or best_params.get(\"n_estimators\") or 2500\n",
    "best_params.pop(\"metric\", None)\n",
    "best_params.pop(\"feature_pre_filter\", None)\n",
    "best_params.pop(\"num_boost_round\", None)\n",
    "best_params.update({\n",
    "    \"n_estimators\": best_iter,\n",
    "    \"random_state\": SEED,\n",
    "    \"n_jobs\": -1,\n",
    "})\n",
    "print(\"[Model B] Mejores parametros:\", best_params)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"[Model B] Fold {fold+1}/{N_SPLITS}\")\n",
    "    X_train = train_df.iloc[train_idx][num_cols]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = train_df.iloc[val_idx][num_cols]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    fold_dir = LGB_MODEL_DIR / f\"fold_{fold}\"\n",
    "    legacy_fold_dir = LGB_MODEL_DIR_LEGACY / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    legacy_fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = lgb.LGBMClassifier(**best_params, random_state=SEED + fold)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "    )\n",
    "\n",
    "    val_preds = model.predict_proba(X_val, num_iteration=model.best_iteration_)[:, 1]\n",
    "    fold_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"[Model B] Fold {fold+1} AUC: {fold_auc:.5f} (best_iter={model.best_iteration_})\")\n",
    "\n",
    "    oof_lgb[val_idx] = val_preds\n",
    "    lgb_models.append(model)\n",
    "\n",
    "    model.booster_.save_model(str(fold_dir / \"best.txt\"), num_iteration=model.best_iteration_)\n",
    "    model.booster_.save_model(str(legacy_fold_dir / \"best.txt\"), num_iteration=model.best_iteration_)\n",
    "    lgb_model_paths.append(fold_dir / \"best.txt\")\n",
    "    lgb_fold_auc.append(fold_auc)\n",
    "\n",
    "lgb_oof_auc = roc_auc_score(y, oof_lgb)\n",
    "print(f\"Model B OOF ROC-AUC: {lgb_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_lgb\": oof_lgb}).to_csv(OOF_DIR / \"oof_lgb.csv\", index=False)\n",
    "pd.DataFrame({\"fold\": np.arange(N_SPLITS), \"fold_auc\": lgb_fold_auc}).to_csv(\n",
    "    OOF_DIR / \"oof_lgb_folds.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available for Model C: True\n",
      "====================================\n",
      "[Model C] Training on GPU with RAPIDS cuML\n",
      "====================================\n",
      "[Model C] Fold 1/5\n",
      "[Model C] GPU training failed, falling back to CPU TF-IDF+SGD. Error: std::bad_alloc: out_of_memory: CUDA error (failed to allocate 7152403344 bytes) at: /__w/rmm/rmm/cpp/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory\n",
      "Model C OOF ROC-AUC: 0.50000\n",
      "[Model C] Warning: expected 5 fold AUCs, got 0; padding with NaN\n"
     ]
    }
   ],
   "source": [
    "# Model C: TF-IDF (char n-grams) + SGD (GPU via RAPIDS cuML when possible)\n",
    "# VRAM-safe defaults; GPU disabled by default to avoid OOM/compat issues.\n",
    "force_cpu_sgd = True  # set to False to try GPU path\n",
    "use_gpu_sgd = torch.cuda.is_available() and not force_cpu_sgd\n",
    "gpu_trim_chars = 400  # truncate text for GPU path to reduce n-grams\n",
    "gpu_ngram_range = (3, 3)\n",
    "gpu_min_df = 5\n",
    "gpu_max_features = 80000\n",
    "cpu_trim_chars = 800  # truncate text for CPU TF-IDF to keep vocab smaller\n",
    "cpu_ngram_range = (3, 4)\n",
    "cpu_min_df = 3\n",
    "cpu_max_features = 200000\n",
    "cpu_n_jobs = -1  # use all CPU cores for parallel folds\n",
    "try:\n",
    "    cp.cuda.runtime.getDeviceCount()\n",
    "except Exception as e:\n",
    "    print(f\"[Model C] GPU not usable (CuPy/CUDA check failed): {e}\")\n",
    "    use_gpu_sgd = False\n",
    "\n",
    "oof_sgd = np.zeros(len(train_df))\n",
    "sgd_models = []\n",
    "sgd_model_paths = []\n",
    "sgd_fold_auc = []\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"[Model C] Training on GPU with RAPIDS cuML\")\n",
    "print(\"====================================\")\n",
    "\n",
    "if use_gpu_sgd:\n",
    "    try:\n",
    "        for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "            print(f\"[Model C] Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "            train_text = train_df.iloc[train_idx][text_col].astype(str)\n",
    "            val_text = train_df.iloc[val_idx][text_col].astype(str)\n",
    "            if gpu_trim_chars is not None:\n",
    "                train_text = train_text.str.slice(stop=gpu_trim_chars)\n",
    "                val_text = val_text.str.slice(stop=gpu_trim_chars)\n",
    "\n",
    "            X_train_gpu = cudf.Series(train_text.values)\n",
    "            X_val_gpu = cudf.Series(val_text.values)\n",
    "\n",
    "            y_train_gpu = cp.array(y[train_idx], dtype=cp.float32)\n",
    "            y_val = y[val_idx]\n",
    "\n",
    "            fold_dir = SGD_MODEL_DIR / f\"fold_{fold}\"\n",
    "            fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            tfidf_gpu = CuTfidfVectorizer(\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=gpu_ngram_range,\n",
    "                min_df=gpu_min_df,\n",
    "                max_features=gpu_max_features,\n",
    "            )\n",
    "\n",
    "            X_train_tfidf = tfidf_gpu.fit_transform(X_train_gpu)\n",
    "            X_val_tfidf = tfidf_gpu.transform(X_val_gpu)\n",
    "\n",
    "            clf = MBSGDClassifier(\n",
    "                loss=\"log\",\n",
    "                penalty=\"l2\",\n",
    "                alpha=1e-4,\n",
    "                epochs=2000,\n",
    "                tol=1e-3,\n",
    "                learning_rate=\"adaptive\",\n",
    "            )\n",
    "\n",
    "            clf.fit(X_train_tfidf, y_train_gpu)\n",
    "\n",
    "            val_probs_gpu = clf.predict_proba(X_val_tfidf)\n",
    "            val_preds = val_probs_gpu.values[:, 1].get() if hasattr(val_probs_gpu, \"values\") else val_probs_gpu[:, 1].get()\n",
    "\n",
    "            fold_auc = roc_auc_score(y_val, val_preds)\n",
    "            print(f\"[Model C] Fold {fold+1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "            oof_sgd[val_idx] = val_preds\n",
    "            sgd_models.append(clf)\n",
    "            joblib.dump(clf, fold_dir / \"best.joblib\")\n",
    "            sgd_model_paths.append(fold_dir / \"best.joblib\")\n",
    "            sgd_fold_auc.append(fold_auc)\n",
    "\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    except Exception as e:\n",
    "        print(f\"[Model C] GPU training failed, falling back to CPU TF-IDF+SGD. Error: {e}\")\n",
    "        oof_sgd = np.zeros(len(train_df))\n",
    "        sgd_models = []\n",
    "        sgd_model_paths = []\n",
    "        sgd_fold_auc = []\n",
    "        use_gpu_sgd = False\n",
    "\n",
    "if not use_gpu_sgd:\n",
    "    print(\"====================================\")\n",
    "    print(\"[Model C] Training on CPU with sklearn TF-IDF + SGDClassifier\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    def train_cpu_fold(fold, train_idx, val_idx):\n",
    "        print(f\"[Model C-CPU] Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "        X_train_cpu = train_df.iloc[train_idx][text_col].astype(str)\n",
    "        X_val_cpu = train_df.iloc[val_idx][text_col].astype(str)\n",
    "        if cpu_trim_chars is not None:\n",
    "            X_train_cpu = X_train_cpu.str.slice(stop=cpu_trim_chars)\n",
    "            X_val_cpu = X_val_cpu.str.slice(stop=cpu_trim_chars)\n",
    "        y_val = y[val_idx]\n",
    "\n",
    "        tfidf_cpu = TfidfVectorizer(\n",
    "            analyzer=\"char\",\n",
    "            ngram_range=cpu_ngram_range,\n",
    "            min_df=cpu_min_df,\n",
    "            max_features=cpu_max_features,\n",
    "        )\n",
    "\n",
    "        X_train_tfidf = tfidf_cpu.fit_transform(X_train_cpu)\n",
    "        X_val_tfidf = tfidf_cpu.transform(X_val_cpu)\n",
    "\n",
    "        clf = SGDClassifier(\n",
    "            loss=\"log_loss\",\n",
    "            penalty=\"l2\",\n",
    "            alpha=1e-4,\n",
    "            max_iter=2000,\n",
    "            tol=1e-3,\n",
    "            random_state=SEED + fold,\n",
    "            learning_rate=\"adaptive\",\n",
    "            eta0=0.1,  # adaptive schedule requires eta0 > 0\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train_tfidf, y[train_idx])\n",
    "        val_preds = clf.predict_proba(X_val_tfidf)[:, 1]\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val, val_preds)\n",
    "        print(f\"[Model C-CPU] Fold {fold+1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "        fold_dir = SGD_MODEL_DIR / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(clf, fold_dir / \"best.joblib\")\n",
    "\n",
    "        return {\n",
    "            \"fold\": fold,\n",
    "            \"val_idx\": val_idx,\n",
    "            \"val_preds\": val_preds,\n",
    "            \"clf\": clf,\n",
    "            \"fold_auc\": fold_auc,\n",
    "            \"model_path\": fold_dir / \"best.joblib\",\n",
    "        }\n",
    "\n",
    "    cpu_results = joblib.Parallel(n_jobs=cpu_n_jobs, backend=\"loky\")(\n",
    "        joblib.delayed(train_cpu_fold)(fold, train_idx, val_idx)\n",
    "        for fold, (train_idx, val_idx) in enumerate(folds)\n",
    "    )\n",
    "\n",
    "    for res in sorted(cpu_results, key=lambda r: r[\"fold\"]):\n",
    "        oof_sgd[res[\"val_idx\"]] = res[\"val_preds\"]\n",
    "        sgd_models.append(res[\"clf\"])\n",
    "        sgd_model_paths.append(res[\"model_path\"])\n",
    "        sgd_fold_auc.append(res[\"fold_auc\"])\n",
    "\n",
    "sgd_oof_auc = roc_auc_score(y, oof_sgd)\n",
    "print(f\"Model C OOF ROC-AUC: {sgd_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_sgd\": oof_sgd}).to_csv(OOF_DIR / \"oof_sgd.csv\", index=False)\n",
    "fold_ids = list(range(len(sgd_fold_auc)))\n",
    "fold_auc_values = list(sgd_fold_auc)\n",
    "if len(fold_auc_values) != N_SPLITS:\n",
    "    print(f\"[Model C] Warning: expected {N_SPLITS} fold AUCs, got {len(fold_auc_values)}; padding with NaN\")\n",
    "    for missing_fold in range(len(fold_auc_values), N_SPLITS):\n",
    "        fold_ids.append(missing_fold)\n",
    "        fold_auc_values.append(np.nan)\n",
    "\n",
    "sgd_folds_df = pd.DataFrame({\"fold\": fold_ids, \"fold_auc\": fold_auc_values})\n",
    "sgd_folds_df.to_csv(OOF_DIR / \"oof_sgd_folds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d88061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Ensemble: Logistic Regression meta-learner on OOF probabilities\n",
    "# Use cached OOF predictions so this cell can run without retraining base models.\n",
    "def load_oof_preds(name):\n",
    "    existing = globals().get(f\"oof_{name}\")\n",
    "    if existing is not None and len(existing):\n",
    "        return np.asarray(existing)\n",
    "    path = OOF_DIR / f\"oof_{name}.csv\"\n",
    "    if path.exists():\n",
    "        col = f\"oof_{name}\"\n",
    "        df = pd.read_csv(path)\n",
    "        if col in df:\n",
    "            print(f\"[Meta] Loaded {col} from {path}\")\n",
    "            return df[col].values\n",
    "    return None\n",
    "\n",
    "oof_sources = {}\n",
    "missing_oof = []\n",
    "length_mismatch = []\n",
    "for key in STACK_BASE_MODELS:\n",
    "    preds = load_oof_preds(key)\n",
    "    if preds is None:\n",
    "        missing_oof.append(key)\n",
    "        continue\n",
    "    if len(preds) != len(y):\n",
    "        length_mismatch.append((key, len(preds)))\n",
    "        continue\n",
    "    oof_sources[key] = preds\n",
    "\n",
    "if missing_oof:\n",
    "    missing_msg = \", \".join(missing_oof)\n",
    "    raise RuntimeError(\n",
    "        f\"Missing OOF predictions for: {missing_msg}. \"\n",
    "        \"Run the corresponding training cells once to cache them to disk.\"\n",
    "    )\n",
    "\n",
    "if length_mismatch:\n",
    "    mismatch_msg = \"; \".join(f\"{k} has {n} rows\" for k, n in length_mismatch)\n",
    "    raise RuntimeError(f\"OOF size mismatch ({len(y)} rows expected): {mismatch_msg}\")\n",
    "\n",
    "stack_train = np.column_stack([oof_sources[k] for k in STACK_BASE_MODELS])\n",
    "\n",
    "# Small CV search to keep the meta-learner from overfitting to a single base model\n",
    "meta_candidates = [0.5, 1.0, 2.0, 4.0]\n",
    "meta_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "best_c = None\n",
    "best_cv_auc = -np.inf\n",
    "\n",
    "for c in meta_candidates:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in meta_skf.split(stack_train, y):\n",
    "        clf = LogisticRegression(\n",
    "            max_iter=500,\n",
    "            n_jobs=-1,\n",
    "            C=c,\n",
    "            solver=\"lbfgs\",\n",
    "        )\n",
    "        clf.fit(stack_train[train_idx], y[train_idx])\n",
    "        preds = clf.predict_proba(stack_train[val_idx])[:, 1]\n",
    "        scores.append(roc_auc_score(y[val_idx], preds))\n",
    "    mean_auc = float(np.mean(scores))\n",
    "    print(f\"[Meta] C={c} CV AUC={mean_auc:.5f}\")\n",
    "    if mean_auc > best_cv_auc:\n",
    "        best_cv_auc = mean_auc\n",
    "        best_c = c\n",
    "\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    C=best_c,\n",
    "    solver=\"lbfgs\",\n",
    ")\n",
    "meta_learner.fit(stack_train, y)\n",
    "meta_learner.base_model_order = STACK_BASE_MODELS\n",
    "stack_auc = roc_auc_score(y, meta_learner.predict_proba(stack_train)[:, 1])\n",
    "print(f\"Meta-learner OOF ROC-AUC: {stack_auc:.5f} (best C={best_c}, cv AUC={best_cv_auc:.5f})\")\n",
    "\n",
    "stack_model_path = STACK_MODEL_DIR / \"meta_learner.joblib\"\n",
    "joblib.dump(meta_learner, stack_model_path)\n",
    "\n",
    "stack_oof = {f\"oof_{k}\": oof_sources[k] for k in STACK_BASE_MODELS}\n",
    "stack_oof[\"oof_stack\"] = meta_learner.predict_proba(stack_train)[:, 1]\n",
    "stack_oof[label_col] = y\n",
    "pd.DataFrame(stack_oof).to_csv(OOF_DIR / \"oof_stack.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e687489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test data (averaging fold predictions for each base model)\n",
    "def _dedupe_paths(paths):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if p.exists() and p not in seen:\n",
    "            seen.add(p)\n",
    "            unique.append(p)\n",
    "    return unique\n",
    "def predict_deberta(df):\n",
    "    tok = globals().get(\"tokenizer\") or AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "    coll = globals().get(\"collator\") or DataCollatorWithPadding(tokenizer=tok, padding=True)\n",
    "    globals()[\"tokenizer\"] = tok\n",
    "    globals()[\"collator\"] = coll\n",
    "    test_ds = HFTextDataset(df, tok, text_col, None, MAX_LENGTH)\n",
    "    fold_paths = _dedupe_paths(\n",
    "        [MODEL_DIR / f\"fold_{f}\" / \"best\" for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"deberta_model_paths\", []))\n",
    "    )\n",
    "    if not fold_paths:\n",
    "        raise RuntimeError(\"No DeBERTa checkpoints found; run Model A training first.\")\n",
    "    fold_preds = []\n",
    "    for path in fold_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "        infer_trainer = Trainer(model=model, tokenizer=tok, data_collator=coll)\n",
    "        preds = infer_trainer.predict(test_ds).predictions\n",
    "        fold_preds.append(softmax_logits(preds))\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def predict_lgb(df):\n",
    "    feats = df[num_cols]\n",
    "    lgb_dirs = [LGB_MODEL_DIR]\n",
    "    if \"LGB_MODEL_DIR_LEGACY\" in globals():\n",
    "        lgb_dirs.append(LGB_MODEL_DIR_LEGACY)\n",
    "\n",
    "    candidate_paths = _dedupe_paths(\n",
    "        [d / f\"fold_{f}\" / \"best.txt\" for d in lgb_dirs for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"lgb_model_paths\", []))\n",
    "    )\n",
    "    if candidate_paths:\n",
    "        models = [lgb.Booster(model_file=str(p)) for p in candidate_paths]\n",
    "    else:\n",
    "        models = globals().get(\"lgb_models\")\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No LightGBM models found; run Model B training first.\")\n",
    "\n",
    "    fold_preds = []\n",
    "    for m in models:\n",
    "        if isinstance(m, lgb.Booster):\n",
    "            fold_preds.append(m.predict(feats))\n",
    "        else:\n",
    "            fold_preds.append(m.predict_proba(feats, num_iteration=getattr(m, \"best_iteration_\", None))[:, 1])\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def predict_sgd(df):\n",
    "    texts = df[text_col].astype(str)\n",
    "    candidate_paths = _dedupe_paths(\n",
    "        [SGD_MODEL_DIR / f\"fold_{f}\" / \"best.joblib\" for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"sgd_model_paths\", []))\n",
    "    )\n",
    "    if candidate_paths:\n",
    "        models = [joblib.load(p) for p in candidate_paths]\n",
    "    else:\n",
    "        models = globals().get(\"sgd_models\")\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No TF-IDF+SGD models found; run Model C training first.\")\n",
    "    fold_preds = [m.predict_proba(texts)[:, 1] for m in models]\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def load_meta_model():\n",
    "    saved_path = STACK_MODEL_DIR / \"meta_learner.joblib\"\n",
    "    if saved_path.exists():\n",
    "        return joblib.load(saved_path)\n",
    "    fallback = globals().get(\"meta_learner\")\n",
    "    if fallback is None:\n",
    "        raise RuntimeError(\"Meta-learner not trained yet; run the stacking cell.\")\n",
    "    return fallback\n",
    "if test_df is not None:\n",
    "    print(\"Running inference on test set...\")\n",
    "    base_preds = {\n",
    "        \"deberta\": predict_deberta(test_df),\n",
    "        \"lgb\": predict_lgb(test_df),\n",
    "        \"sgd\": predict_sgd(test_df),\n",
    "    }\n",
    "    meta_for_inference = load_meta_model()\n",
    "    base_order = getattr(meta_for_inference, \"base_model_order\", STACK_BASE_MODELS)\n",
    "    stack_test = np.column_stack([base_preds[name] for name in base_order])\n",
    "    test_pred = meta_for_inference.predict_proba(stack_test)[:, 1]\n",
    "    submission = pd.DataFrame({\"id\": test_df.index, \"prediction\": test_pred})\n",
    "    submission.to_csv(WORK_DIR / \"submission.csv\", index=False)\n",
    "    print(\"Saved submission.csv\")\n",
    "else:\n",
    "    print(\"No test file found; set TEST_PATH to run inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostics summary (OOF and fold-level)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Helper to grab in-memory arrays first, otherwise fall back to saved OOF files\n",
    "\n",
    "def load_oof(name):\n",
    "    in_memory = globals().get(f\"oof_{name}\")\n",
    "    if in_memory is not None:\n",
    "        return np.asarray(in_memory)\n",
    "    path = OOF_DIR / f\"oof_{name}.csv\"\n",
    "    if path.exists():\n",
    "        col = f\"oof_{name}\"\n",
    "        df = pd.read_csv(path)\n",
    "        if col in df:\n",
    "            return df[col].values\n",
    "    return None\n",
    "\n",
    "\n",
    "def fold_scores(preds):\n",
    "    scores = []\n",
    "    for fold, (_, val_idx) in enumerate(folds):\n",
    "        scores.append(roc_auc_score(y[val_idx], preds[val_idx]))\n",
    "    return scores\n",
    "\n",
    "\n",
    "summary_rows = []\n",
    "for label, key in [\n",
    "    (\"Model A: DeBERTa-v3\", \"deberta\"),\n",
    "    (\"Model B: LightGBM numeric\", \"lgb\"),\n",
    "    (\"Model C: TF-IDF + SGD\", \"sgd\"),\n",
    "    (\"Meta-learner (stack)\", \"stack\"),\n",
    "]:\n",
    "    preds = load_oof(key)\n",
    "    if preds is None:\n",
    "        print(f\"Skipping {label}: no OOF predictions found\")\n",
    "        continue\n",
    "    overall = roc_auc_score(y, preds)\n",
    "    folds_auc = fold_scores(preds) if len(preds) == len(y) else None\n",
    "    summary_rows.append(\n",
    "        OrderedDict(\n",
    "            model=label,\n",
    "            overall_auc=overall,\n",
    "            fold_mean=np.mean(folds_auc) if folds_auc else None,\n",
    "            fold_std=np.std(folds_auc) if folds_auc else None,\n",
    "            min_fold=np.min(folds_auc) if folds_auc else None,\n",
    "            max_fold=np.max(folds_auc) if folds_auc else None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "if not summary_df.empty:\n",
    "    display(summary_df.sort_values(\"overall_auc\", ascending=False))\n",
    "else:\n",
    "    print(\"No OOF data available to summarize.\")\n",
    "\n",
    "# Meta-learner diagnostics (if saved/loaded)\n",
    "meta_model = None\n",
    "if 'load_meta_model' in globals():\n",
    "    try:\n",
    "        meta_model = load_meta_model()\n",
    "    except Exception:\n",
    "        meta_model = globals().get('meta_learner')\n",
    "elif 'meta_learner' in globals():\n",
    "    meta_model = meta_learner\n",
    "\n",
    "if meta_model is not None and hasattr(meta_model, 'coef_'):\n",
    "    coef = meta_model.coef_.ravel()\n",
    "    bases = ['deberta', 'lgb', 'sgd'][: len(coef)]\n",
    "    print(\"Meta-learner coefficients (positive -> higher AI probability):\")\n",
    "    display(pd.Series(coef, index=bases))\n",
    "    if hasattr(meta_model, 'C'):\n",
    "        print(f\"Meta-learner C: {getattr(meta_model, 'C', None)}\")\n",
    "\n",
    "# Correlation between base model OOF predictions (helps assess diversity)\n",
    "base_preds = {\n",
    "    name: load_oof(name)\n",
    "    for name in ['deberta', 'lgb', 'sgd']\n",
    "}\n",
    "if all(v is not None for v in base_preds.values()):\n",
    "    corr_df = pd.DataFrame(base_preds)\n",
    "    print(\"Correlation of base model OOF predictions:\")\n",
    "    display(corr_df.corr())\n",
    "\n",
    "# Class balance reminder\n",
    "pos_rate = y.mean()\n",
    "print(f\"Positive rate in training: {pos_rate:.4f} (n={len(y)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
