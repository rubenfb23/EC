{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Generated Text Detection\n",
    "\n",
    "Stacked ensemble (DeBERTa-v3 + LightGBM + TF-IDF/SGD) on `merged_ai_human_multisocial_features_train.csv` / `merged_ai_human_multisocial_features_test.csv` when present (0=Human, 1=AI). Flow: setup -> data check -> Model A/B/C CV (OOF) -> stacking -> inference. Text column: `text`; label column: `label`; numerical features like `burstiness`, `perplexity_score`, `lexical_diversity`, `gunning_fog_index`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2305080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if running in a fresh environment\n",
    "# !pip install -q pandas numpy torch torchvision torchaudio transformers datasets lightgbm scikit-learn\n",
    "# For CUDA builds of PyTorch: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Using training file: /home/ruben/EC/src/ai_vs_human/merged_ai_human_multisocial_features_cleaned_train.csv\n",
      "Using test file: /home/ruben/EC/src/ai_vs_human/merged_ai_human_multisocial_features_cleaned_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as CuTfidfVectorizer\n",
    "from cuml.linear_model import MBSGDClassifier\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "EXTRA_SEEDS = [123, 2029]  # opcional: sumar seeds para ensembles más robustos\n",
    "SEED_LIST = [SEED] + EXTRA_SEEDS\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Notebook config (ajusta aquí en vez de usar variables de entorno)\n",
    "BASE_MODEL = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 512\n",
    "N_SPLITS = 5\n",
    "NUM_EPOCHS = 3\n",
    "STACK_BASE_MODELS = [\"deberta\", \"lgb\", \"sgd\"]\n",
    "\n",
    "DEBERTA_LR = 1.5e-5\n",
    "DEBERTA_WEIGHT_DECAY = 0.05\n",
    "DEBERTA_WARMUP_RATIO = 0.1\n",
    "LABEL_SMOOTHING = 0.05\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "# Multi-GPU / batch settings\n",
    "AVAILABLE_GPUS = list(range(torch.cuda.device_count())) if torch.cuda.is_available() else []\n",
    "REQUESTED_GPU_IDS = []  # ejemplo: [0, 1, 2, 3]; si está vacío usa los disponibles\n",
    "if REQUESTED_GPU_IDS:\n",
    "    AVAILABLE_GPUS = [g for g in REQUESTED_GPU_IDS if g in AVAILABLE_GPUS] or REQUESTED_GPU_IDS\n",
    "PARALLEL_FOLDS = 0  # pon >1 para paralelizar folds\n",
    "if PARALLEL_FOLDS > len(AVAILABLE_GPUS):\n",
    "    print(f\"PARALLEL_FOLDS limited to {len(AVAILABLE_GPUS)} available GPUs\")\n",
    "    PARALLEL_FOLDS = len(AVAILABLE_GPUS)\n",
    "DEBERTA_TRAIN_BATCH = 8\n",
    "DEBERTA_EVAL_BATCH = 16\n",
    "DEBERTA_GRAD_ACCUM = 4\n",
    "DEBERTA_NUM_WORKERS = 2\n",
    "if PARALLEL_FOLDS > 1:\n",
    "    print(f\"Parallel DeBERTa folds enabled: {PARALLEL_FOLDS} workers over GPUs {AVAILABLE_GPUS}\")\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = None  # Path a un CSV específico; None intenta auto-descubrir\n",
    "TEST_PATH_OVERRIDE = None  # Path a test; None para autodetectar\n",
    "\n",
    "# Preprocesado/ajustes varios\n",
    "LOG1P_THRESHOLD = 1000.0\n",
    "GROUP_COLUMN = None  # e.g. \"source\" para GroupKFold\n",
    "LGB_TIME_BUDGET = 0  # segundos; 0 = sin límite\n",
    "CPU_TRIM_CHARS = 1200\n",
    "CPU_MAX_FEATURES = 300000\n",
    "TFIDF_LOGREG = True\n",
    "\n",
    "\n",
    "CWD = Path.cwd()\n",
    "data_override = Path(DATA_PATH).expanduser() if DATA_PATH else None\n",
    "candidate_paths = [\n",
    "    data_override,\n",
    "    CWD / \"merged_ai_human_multisocial_features_cleaned_train.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features_cleaned_train.csv\",\n",
    "    CWD / \"merged_ai_human_multisocial_features_train.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features_train.csv\",\n",
    "    CWD / \"merged_ai_human_multisocial_features_cleaned.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features_cleaned.csv\",\n",
    "    CWD / \"merged_ai_human_multisocial_features.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features.csv\",\n",
    "    CWD / \"ai_human_content_detection_dataset.csv\",\n",
    "    CWD / \"src/ai_vs_human/ai_human_content_detection_dataset.csv\",\n",
    "]\n",
    "candidate_paths = [p for p in candidate_paths if p is not None]\n",
    "for candidate_path in candidate_paths:\n",
    "    if candidate_path.exists():\n",
    "        DATA_PATH = candidate_path\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"No training data file found. Set DATA_PATH or place merged_ai_human_multisocial_features_train.csv (or merged_ai_human_multisocial_features.csv / ai_human_content_detection_dataset.csv) in the repo.\"\n",
    "    )\n",
    "print(f\"Using training file: {DATA_PATH}\")\n",
    "\n",
    "WORK_DIR = DATA_PATH.parent\n",
    "MODEL_DIR = WORK_DIR / \"models\" / \"deberta_v3_base\"\n",
    "LGB_MODEL_DIR = WORK_DIR / \"models\" / \"lightgbm\"\n",
    "LGB_MODEL_DIR_LEGACY = WORK_DIR / \"models\" / \"lightgbm_numeric\"\n",
    "SGD_MODEL_DIR = WORK_DIR / \"models\" / \"tfidf_sgd\"\n",
    "STACK_MODEL_DIR = WORK_DIR / \"models\" / \"stack_meta\"\n",
    "for path in (MODEL_DIR, LGB_MODEL_DIR, LGB_MODEL_DIR_LEGACY, SGD_MODEL_DIR, STACK_MODEL_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR = WORK_DIR / \"oof\"\n",
    "OOF_DIR.mkdir(exist_ok=True)\n",
    "paired_test = None\n",
    "if DATA_PATH.suffix:\n",
    "    name = DATA_PATH.name\n",
    "    if name.endswith(\"_train\" + DATA_PATH.suffix):\n",
    "        paired_test = DATA_PATH.with_name(name.replace(\"_train\", \"_test\"))\n",
    "    else:\n",
    "        paired_test = DATA_PATH.with_name(DATA_PATH.stem + \"_test\" + DATA_PATH.suffix)\n",
    "test_override = Path(TEST_PATH_OVERRIDE).expanduser() if TEST_PATH_OVERRIDE else None\n",
    "test_candidates = [\n",
    "    test_override,\n",
    "    paired_test,\n",
    "    WORK_DIR / \"merged_ai_human_multisocial_features_test.csv\",\n",
    "    WORK_DIR / \"ai_human_content_detection_test.csv\",\n",
    "    WORK_DIR / \"ai_human_content_detection_dataset.csv\",\n",
    "    DATA_PATH,  # fallback: reuse train data so inference still runs\n",
    "]\n",
    "test_candidates = [p for p in test_candidates if p is not None]\n",
    "TEST_PATH = next((p for p in test_candidates if p.exists()), test_candidates[0])\n",
    "print(f\"Using test file: {TEST_PATH}\")\n",
    "if TEST_PATH == DATA_PATH:\n",
    "    print(\"TEST_PATH not provided; using training data as a smoke-test for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af3fa7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied log1p to skewed columns: ['character_count']\n",
      "Train shape: (464171, 52)\n",
      "Numeric features (13): ['word_count', 'character_count', 'sentence_count', 'lexical_diversity', 'avg_sentence_length', 'avg_word_length', 'punctuation_ratio', 'flesch_reading_ease', 'gunning_fog_index', 'passive_voice_ratio']...\n",
      "                                                text  label\n",
      "0  Cars. Cars have been around since they became ...      0\n",
      "1  Transportation is a large necessity in most co...      0\n",
      "                        count        mean         std       min         25%  \\\n",
      "word_count           464171.0  396.088289  162.141661  3.000000  283.000000   \n",
      "character_count      464171.0    7.634398    0.423063  2.708050    7.367709   \n",
      "sentence_count       464171.0   20.511417    8.551840  1.000000   14.000000   \n",
      "lexical_diversity    464171.0    0.450273    0.089061  0.277778    0.388350   \n",
      "avg_sentence_length  464171.0   20.170071    5.702400  2.000000   16.500000   \n",
      "\n",
      "                            50%         75%         max  \n",
      "word_count           368.000000  477.000000  873.000000  \n",
      "character_count        7.649693    7.907284    8.516993  \n",
      "sentence_count        20.000000   25.000000   44.000000  \n",
      "lexical_diversity      0.443316    0.504816    1.000000  \n",
      "avg_sentence_length   19.294118   22.470588   42.044419  \n",
      "Top correlated numeric features with label:\n",
      "avg_word_length         0.567862\n",
      "flesch_reading_ease    -0.496188\n",
      "burstiness             -0.384371\n",
      "gunning_fog_index       0.335020\n",
      "word_count             -0.226994\n",
      "sentiment_score         0.200581\n",
      "predictability_score   -0.186563\n",
      "lexical_diversity       0.186563\n",
      "sentence_count         -0.160569\n",
      "avg_sentence_length    -0.127952\n",
      "Name: label, dtype: float64\n",
      "Using GroupKFold on `model_human` with reduced splits=2 (n_groups=2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data and inspect\n",
    "train_df = pd.read_csv(DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "alt_text_cols = (\"text_content\",)\n",
    "meta_prefixes = (\"src_\", \"lang_\", \"model_\", \"ds_\")  # exclude metadata/lang/model/dataset to avoid leakage\n",
    "\n",
    "if text_col not in train_df.columns:\n",
    "    for alt_col in alt_text_cols:\n",
    "        if alt_col in train_df.columns:\n",
    "            train_df = train_df.rename(columns={alt_col: text_col})\n",
    "            break\n",
    "if text_col not in train_df.columns:\n",
    "    raise ValueError(f\"Training data missing `{text_col}` column; set DATA_PATH to a file containing text.\")\n",
    "\n",
    "# Drop exact duplicate text+label rows to reduce leakage\n",
    "initial_rows = len(train_df)\n",
    "train_df = train_df.drop_duplicates(subset=[text_col, label_col]).reset_index(drop=True)\n",
    "if len(train_df) != initial_rows:\n",
    "    print(f\"Dropped {initial_rows - len(train_df)} duplicate rows (text+label)\")\n",
    "\n",
    "# Optional: normalize skewed numeric columns later\n",
    "log1p_threshold = LOG1P_THRESHOLD\n",
    "drop_numeric = {\"grammar_errors\", \"length\"}\n",
    "num_cols = [\n",
    "    c\n",
    "    for c in train_df.columns\n",
    "    if c not in [text_col, label_col]\n",
    "    and c not in drop_numeric\n",
    "    and pd.api.types.is_numeric_dtype(train_df[c])\n",
    "    and not any(c.startswith(pref) for pref in meta_prefixes)\n",
    "]\n",
    "\n",
    "train_df[num_cols] = train_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "num_medians = train_df[num_cols].median()\n",
    "train_df[num_cols] = train_df[num_cols].fillna(num_medians)\n",
    "if test_df is not None:\n",
    "    if text_col not in test_df.columns:\n",
    "        for alt_col in alt_text_cols:\n",
    "            if alt_col in test_df.columns:\n",
    "                test_df = test_df.rename(columns={alt_col: text_col})\n",
    "                break\n",
    "    if text_col not in test_df.columns:\n",
    "        raise ValueError(f\"Test data missing `{text_col}` column; set TEST_PATH to a file containing text.\")\n",
    "    missing_num_cols = [c for c in num_cols if c not in test_df.columns]\n",
    "    for col in missing_num_cols:\n",
    "        test_df[col] = num_medians[col]\n",
    "    if missing_num_cols:\n",
    "        print(f\"Filled missing numeric columns in test set: {len(missing_num_cols)} (using train medians)\")\n",
    "    test_df[num_cols] = test_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(num_medians)\n",
    "\n",
    "# Log1p transform for heavily skewed positive numeric features\n",
    "log1p_cols = [\n",
    "    c\n",
    "    for c in num_cols\n",
    "    if (train_df[c] > 0).all()\n",
    "    and train_df[c].max() > log1p_threshold\n",
    "]\n",
    "if log1p_cols:\n",
    "    train_df[log1p_cols] = np.log1p(train_df[log1p_cols])\n",
    "    if test_df is not None:\n",
    "        test_df[log1p_cols] = np.log1p(test_df[log1p_cols])\n",
    "    preview = log1p_cols[:5]\n",
    "    print(f\"Applied log1p to skewed columns: {preview}{'...' if len(log1p_cols) > 5 else ''}\")\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Numeric features ({len(num_cols)}): {num_cols[:10]}{'...' if len(num_cols) > 10 else ''}\")\n",
    "print(train_df[[text_col, label_col]].head(2))\n",
    "print(train_df[num_cols].describe().T.head())\n",
    "\n",
    "# Quick correlation check to spot potential leakage/high-signal numeric features\n",
    "corr = (\n",
    "    train_df[num_cols + [label_col]]\n",
    "    .corr()[label_col]\n",
    "    .drop(label_col)\n",
    "    .sort_values(key=np.abs, ascending=False)\n",
    ")\n",
    "print(\"Top correlated numeric features with label:\")\n",
    "print(corr.head(10))\n",
    "\n",
    "y = train_df[label_col].astype(int).values\n",
    "\n",
    "# Prefer GroupKFold if there is a dataset/source column to avoid leakage\n",
    "preferred_group_col = GROUP_COLUMN\n",
    "\n",
    "def detect_group_column(df):\n",
    "    if preferred_group_col and preferred_group_col in df.columns:\n",
    "        return preferred_group_col\n",
    "    for col in df.columns:\n",
    "        if any(col.startswith(pref) for pref in meta_prefixes):\n",
    "            nunique = df[col].nunique()\n",
    "            if 1 < nunique < len(df):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "_group_col = detect_group_column(train_df)\n",
    "if _group_col:\n",
    "    groups = train_df[_group_col]\n",
    "    n_groups = groups.nunique()\n",
    "    if n_groups >= N_SPLITS:\n",
    "        cv_splitter = GroupKFold(n_splits=N_SPLITS)\n",
    "        folds = list(cv_splitter.split(train_df[text_col], y, groups))\n",
    "        print(f\"Using GroupKFold on column `{_group_col}` (n_groups={n_groups})\")\n",
    "    elif n_groups > 1:\n",
    "        cv_splitter = GroupKFold(n_splits=n_groups)\n",
    "        folds = list(cv_splitter.split(train_df[text_col], y, groups))\n",
    "        print(f\"Using GroupKFold on `{_group_col}` with reduced splits={n_groups} (n_groups={n_groups})\")\n",
    "    else:\n",
    "        cv_splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        folds = list(cv_splitter.split(train_df[text_col], y))\n",
    "        print(f\"Group column `{_group_col}` has <=1 group; falling back to StratifiedKFold\")\n",
    "else:\n",
    "    cv_splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    folds = list(cv_splitter.split(train_df[text_col], y))\n",
    "    print(\"Using StratifiedKFold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c62fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper classes and metrics\n",
    "class HFTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, text_col, label_col=None, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.loc[idx, self.text_col])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "        if self.label_col is not None:\n",
    "            enc[\"labels\"] = int(self.df.loc[idx, self.label_col])\n",
    "        return enc\n",
    "\n",
    "\n",
    "def softmax_logits(logits):\n",
    "    logits = torch.tensor(logits)\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs[:, 1]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = softmax_logits(logits)\n",
    "    return {\"roc_auc\": roc_auc_score(labels, probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bc9f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model A] Preparing fold 1/5\n",
      "[Model A] Preparing fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_168598/426247727.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='278' max='11240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  278/11240 09:46 < 6:28:22, 0.47 it/s, Epoch 0.10/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m jobs:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         fold, val_idx, preds, best_dir, device_id = \u001b[43m_train_deberta_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Model A] Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_SPLITS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (GPU \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m         oof_deberta[val_idx] = preds\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36m_train_deberta_fold\u001b[39m\u001b[34m(job)\u001b[39m\n\u001b[32m    107\u001b[39m     args.load_best_model_at_end = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    109\u001b[39m trainer = Trainer(\n\u001b[32m    110\u001b[39m     model=model,\n\u001b[32m    111\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m     callbacks=[EarlyStoppingCallback(early_stopping_patience=\u001b[32m1\u001b[39m)],\n\u001b[32m    118\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m preds = trainer.predict(val_ds).predictions\n\u001b[32m    123\u001b[39m oof_preds = softmax_logits(preds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:121\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    119\u001b[39m         thread.start()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    123\u001b[39m     _worker(\u001b[32m0\u001b[39m, modules[\u001b[32m0\u001b[39m], inputs[\u001b[32m0\u001b[39m], kwargs_tup[\u001b[32m0\u001b[39m], devices[\u001b[32m0\u001b[39m], streams[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1147\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1167\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1168\u001b[39m         lock.release()\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Model A: DeBERTa-v3-base with Stratified/Group K-Fold OOF (optionally multi-GPU parallel across folds)\n",
    "import multiprocessing as mp\n",
    "\n",
    "def _train_deberta_fold(job):\n",
    "    fold = job[\"fold\"]\n",
    "    device_id = job.get(\"device_id\")\n",
    "    train_slice = job[\"train_df\"]\n",
    "    val_slice = job[\"val_df\"]\n",
    "    val_idx = job[\"val_idx\"]\n",
    "\n",
    "    if device_id is not None:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(0)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    fold_dir = MODEL_DIR / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer_local = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "    collator_local = DataCollatorWithPadding(tokenizer=tokenizer_local, padding=True)\n",
    "\n",
    "    train_ds = HFTextDataset(train_slice, tokenizer_local, text_col, label_col, MAX_LENGTH)\n",
    "    val_ds = HFTextDataset(val_slice, tokenizer_local, text_col, label_col, MAX_LENGTH)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, num_labels=2\n",
    "    )\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        try:\n",
    "            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "        except TypeError:\n",
    "            try:\n",
    "                model.gradient_checkpointing_enable(use_reentrant=False)\n",
    "            except TypeError:\n",
    "                model.gradient_checkpointing_enable()\n",
    "        if getattr(model, \"config\", None) is not None and hasattr(model.config, \"use_cache\"):\n",
    "            model.config.use_cache = False\n",
    "\n",
    "    training_kwargs = dict(\n",
    "        output_dir=str(fold_dir),\n",
    "        per_device_train_batch_size=DEBERTA_TRAIN_BATCH,\n",
    "        per_device_eval_batch_size=DEBERTA_EVAL_BATCH,\n",
    "        gradient_accumulation_steps=DEBERTA_GRAD_ACCUM,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=DEBERTA_LR,\n",
    "        weight_decay=DEBERTA_WEIGHT_DECAY,\n",
    "        warmup_ratio=DEBERTA_WARMUP_RATIO,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_num_workers=DEBERTA_NUM_WORKERS,\n",
    "        report_to=\"none\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        label_smoothing_factor=LABEL_SMOOTHING,\n",
    "        gradient_checkpointing=False,  # manual GC above with use_reentrant=False to avoid double-backward issues\n",
    "        ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "    # Handle TrainingArguments API differences across transformers versions\n",
    "    try:\n",
    "        args = TrainingArguments(\n",
    "            **training_kwargs,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        try:\n",
    "            args = TrainingArguments(\n",
    "                **training_kwargs,\n",
    "                evaluate_during_training=True,\n",
    "                eval_steps=500,\n",
    "                save_steps=500,\n",
    "            )\n",
    "        except TypeError:\n",
    "            fallback_kwargs = training_kwargs.copy()\n",
    "            for key in (\n",
    "                \"load_best_model_at_end\",\n",
    "                \"metric_for_best_model\",\n",
    "                \"greater_is_better\",\n",
    "                \"lr_scheduler_type\",\n",
    "                \"label_smoothing_factor\",\n",
    "                \"gradient_checkpointing\",\n",
    "                \"ddp_find_unused_parameters\",\n",
    "            ):\n",
    "                fallback_kwargs.pop(key, None)\n",
    "            args = TrainingArguments(**fallback_kwargs)\n",
    "\n",
    "    # Ensure metric_for_best_model exists for EarlyStopping across transformers versions\n",
    "    if getattr(args, \"metric_for_best_model\", None) is None:\n",
    "        args.metric_for_best_model = \"roc_auc\"\n",
    "    if getattr(args, \"greater_is_better\", None) is None:\n",
    "        args.greater_is_better = True\n",
    "    # Force evaluation/save strategy to something valid (epoch)\n",
    "    if getattr(args, \"evaluation_strategy\", None) in (None, \"no\", \"none\"):\n",
    "        args.evaluation_strategy = \"epoch\"\n",
    "    # Some versions expose eval_strategy alias used in Trainer internals\n",
    "    if getattr(args, \"eval_strategy\", None) in (None, \"no\", \"none\"):\n",
    "        args.eval_strategy = getattr(args, \"evaluation_strategy\", \"epoch\")\n",
    "    if getattr(args, \"save_strategy\", None) in (None, \"no\", \"none\"):\n",
    "        args.save_strategy = getattr(args, \"evaluation_strategy\", \"epoch\")\n",
    "    if getattr(args, \"load_best_model_at_end\", None) is None:\n",
    "        args.load_best_model_at_end = True\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer_local,\n",
    "        data_collator=collator_local,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    preds = trainer.predict(val_ds).predictions\n",
    "    oof_preds = softmax_logits(preds)\n",
    "\n",
    "    best_dir = trainer.state.best_model_checkpoint or str(fold_dir / \"best\")\n",
    "    if trainer.state.best_model_checkpoint is None:\n",
    "        trainer.save_model(best_dir)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return fold, val_idx, oof_preds, best_dir, device_id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "oof_deberta = np.zeros(len(train_df))\n",
    "deverta_model_paths = [None] * len(folds)\n",
    "\n",
    "jobs = []\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"[Model A] Preparing fold {fold+1}/{N_SPLITS}\")\n",
    "    train_slice = train_df.iloc[train_idx][[text_col, label_col]].reset_index(drop=True)\n",
    "    val_slice = train_df.iloc[val_idx][[text_col, label_col]].reset_index(drop=True)\n",
    "    device_id = None\n",
    "    if PARALLEL_FOLDS and len(AVAILABLE_GPUS):\n",
    "        device_id = AVAILABLE_GPUS[fold % len(AVAILABLE_GPUS)]\n",
    "    jobs.append({\"fold\": fold, \"train_df\": train_slice, \"val_df\": val_slice, \"val_idx\": val_idx, \"device_id\": device_id})\n",
    "\n",
    "if PARALLEL_FOLDS > 1 and len(AVAILABLE_GPUS):\n",
    "    max_workers = min(PARALLEL_FOLDS, len(AVAILABLE_GPUS), len(jobs))\n",
    "    print(f\"[Model A] Running folds in parallel on GPUs {AVAILABLE_GPUS} (workers={max_workers})\")\n",
    "    ctx = mp.get_context(\"spawn\")\n",
    "    with ctx.Pool(processes=max_workers) as pool:\n",
    "        for fold, val_idx, preds, best_dir, device_id in pool.imap_unordered(_train_deberta_fold, jobs):\n",
    "            print(f\"[Model A] Fold {fold+1} finished on GPU {device_id}\")\n",
    "            oof_deberta[val_idx] = preds\n",
    "            deverta_model_paths[fold] = best_dir\n",
    "else:\n",
    "    for job in jobs:\n",
    "        fold, val_idx, preds, best_dir, device_id = _train_deberta_fold(job)\n",
    "        print(f\"[Model A] Fold {fold+1}/{N_SPLITS} (GPU {device_id})\")\n",
    "        oof_deberta[val_idx] = preds\n",
    "        deverta_model_paths[job[\"fold\"]] = best_dir\n",
    "\n",
    "# Remove potential None placeholders and keep existing behavior\n",
    "_deberta_model_paths_clean = [p for p in deverta_model_paths if p is not None]\n",
    "deverta_model_paths = _deberta_model_paths_clean\n",
    "\n",
    "\n",
    "deverta_oof_auc = roc_auc_score(y, oof_deberta)\n",
    "print(f\"Model A OOF ROC-AUC: {deverta_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_deberta\": oof_deberta}).to_csv(OOF_DIR / \"oof_deberta.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model B: LightGBM on numerical features only (Optuna tuning)\n",
    "import optuna\n",
    "from optuna.integration import lightgbm as lgb_optuna\n",
    "\n",
    "oof_lgb = np.zeros(len(train_df))\n",
    "lgb_models = []\n",
    "lgb_model_paths = []\n",
    "lgb_fold_auc = []\n",
    "\n",
    "time_budget = int(LGB_TIME_BUDGET)\n",
    "\n",
    "# Tuning con los mismos folds que el CV de entrenamiento\n",
    "train_data = lgb.Dataset(train_df[num_cols], label=y)\n",
    "\n",
    "tuner_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"n_estimators\": 3000,\n",
    "    \"learning_rate\": 0.015,\n",
    "    \"verbosity\": -1,\n",
    "    \"feature_pre_filter\": False,\n",
    "    \"scale_pos_weight\": (len(y) - y.sum()) / y.sum(),\n",
    "    \"num_leaves\": 63,\n",
    "    \"max_depth\": -1,\n",
    "    \"min_child_samples\": 40,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"reg_lambda\": 1.5,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"min_split_gain\": 0.0,\n",
    "}\n",
    "\n",
    "tuner_kwargs = dict(\n",
    "    params=tuner_params,\n",
    "    train_set=train_data,\n",
    "    folds=folds,\n",
    "    num_boost_round=2500,\n",
    "    callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "    return_cvbooster=True,\n",
    "    study=optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED)),\n",
    ")\n",
    "if time_budget > 0:\n",
    "    tuner_kwargs[\"time_budget\"] = time_budget\n",
    "\n",
    "# Ejecuta tuning (ajusta time_budget o usa study.sampler si quieres limitar)\n",
    "tuner = lgb_optuna.LightGBMTunerCV(**tuner_kwargs)\n",
    "tuner.run()\n",
    "\n",
    "best_params = tuner.best_params\n",
    "best_cvbooster = None\n",
    "if hasattr(tuner, \"get_best_booster\"):\n",
    "    try:\n",
    "        best_cvbooster = tuner.get_best_booster()\n",
    "    except Exception:\n",
    "        best_cvbooster = None\n",
    "best_iter = getattr(best_cvbooster, \"best_iteration\", None) if best_cvbooster is not None else None\n",
    "if best_iter is None:\n",
    "    best_iter = best_params.get(\"num_boost_round\") or best_params.get(\"n_estimators\") or 2500\n",
    "best_params.pop(\"metric\", None)\n",
    "best_params.pop(\"feature_pre_filter\", None)\n",
    "best_params.pop(\"num_boost_round\", None)\n",
    "best_params.update({\n",
    "    \"n_estimators\": best_iter,\n",
    "    \"random_state\": SEED,\n",
    "    \"n_jobs\": -1,\n",
    "})\n",
    "print(\"[Model B] Mejores parametros:\", best_params)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"[Model B] Fold {fold+1}/{N_SPLITS}\")\n",
    "    X_train = train_df.iloc[train_idx][num_cols]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = train_df.iloc[val_idx][num_cols]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    fold_dir = LGB_MODEL_DIR / f\"fold_{fold}\"\n",
    "    legacy_fold_dir = LGB_MODEL_DIR_LEGACY / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "    legacy_fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = lgb.LGBMClassifier(**best_params, random_state=SEED + fold)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "    )\n",
    "\n",
    "    val_preds = model.predict_proba(X_val, num_iteration=model.best_iteration_)[:, 1]\n",
    "    fold_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"[Model B] Fold {fold+1} AUC: {fold_auc:.5f} (best_iter={model.best_iteration_})\")\n",
    "\n",
    "    oof_lgb[val_idx] = val_preds\n",
    "    lgb_models.append(model)\n",
    "\n",
    "    model.booster_.save_model(str(fold_dir / \"best.txt\"), num_iteration=model.best_iteration_)\n",
    "    model.booster_.save_model(str(legacy_fold_dir / \"best.txt\"), num_iteration=model.best_iteration_)\n",
    "    lgb_model_paths.append(fold_dir / \"best.txt\")\n",
    "    lgb_fold_auc.append(fold_auc)\n",
    "\n",
    "lgb_oof_auc = roc_auc_score(y, oof_lgb)\n",
    "print(f\"Model B OOF ROC-AUC: {lgb_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_lgb\": oof_lgb}).to_csv(OOF_DIR / \"oof_lgb.csv\", index=False)\n",
    "pd.DataFrame({\"fold\": np.arange(N_SPLITS), \"fold_auc\": lgb_fold_auc}).to_csv(\n",
    "    OOF_DIR / \"oof_lgb_folds.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available for Model C: True\n",
      "====================================\n",
      "[Model C] Training on GPU with RAPIDS cuML\n",
      "====================================\n",
      "[Model C] Fold 1/5\n",
      "[Model C] GPU training failed, falling back to CPU TF-IDF+SGD. Error: std::bad_alloc: out_of_memory: CUDA error (failed to allocate 7152403344 bytes) at: /__w/rmm/rmm/cpp/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory\n",
      "Model C OOF ROC-AUC: 0.50000\n",
      "[Model C] Warning: expected 5 fold AUCs, got 0; padding with NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model C: TF-IDF (char n-grams) + SGD/LogReg (GPU via RAPIDS cuML when possible)\n",
    "# VRAM-safe defaults; GPU disabled by default to avoid OOM/compat issues.\n",
    "force_cpu_sgd = True  # set to False to try GPU path\n",
    "use_gpu_sgd = torch.cuda.is_available() and not force_cpu_sgd\n",
    "gpu_trim_chars = 400  # truncate text for GPU path to reduce n-grams\n",
    "gpu_ngram_range = (3, 5)\n",
    "gpu_min_df = 2\n",
    "gpu_max_features = 80000\n",
    "cpu_trim_chars = int(CPU_TRIM_CHARS)  # more contexto por defecto\n",
    "cpu_ngram_range = (3, 5)\n",
    "cpu_min_df = 2\n",
    "cpu_max_features = int(CPU_MAX_FEATURES)\n",
    "cpu_n_jobs = -1  # use all CPU cores for parallel folds\n",
    "use_logreg_tfidf = bool(TFIDF_LOGREG)  # LogisticRegression sobre TF-IDF (mejor calibración)\n",
    "try:\n",
    "    cp.cuda.runtime.getDeviceCount()\n",
    "except Exception as e:\n",
    "    print(f\"[Model C] GPU not usable (CuPy/CUDA check failed): {e}\")\n",
    "    use_gpu_sgd = False\n",
    "\n",
    "oof_sgd = np.zeros(len(train_df))\n",
    "sgd_models = []\n",
    "sgd_model_paths = []\n",
    "sgd_fold_auc = []\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"[Model C] Training on GPU with RAPIDS cuML\")\n",
    "print(\"====================================\")\n",
    "\n",
    "if use_gpu_sgd:\n",
    "    try:\n",
    "        for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "            print(f\"[Model C] Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "            train_text = train_df.iloc[train_idx][text_col].astype(str)\n",
    "            val_text = train_df.iloc[val_idx][text_col].astype(str)\n",
    "            if gpu_trim_chars is not None:\n",
    "                train_text = train_text.str.slice(stop=gpu_trim_chars)\n",
    "                val_text = val_text.str.slice(stop=gpu_trim_chars)\n",
    "\n",
    "            X_train_gpu = cudf.Series(train_text.values)\n",
    "            X_val_gpu = cudf.Series(val_text.values)\n",
    "\n",
    "            y_train_gpu = cp.array(y[train_idx], dtype=cp.float32)\n",
    "            y_val = y[val_idx]\n",
    "\n",
    "            fold_dir = SGD_MODEL_DIR / f\"fold_{fold}\"\n",
    "            fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            tfidf_gpu = CuTfidfVectorizer(\n",
    "                analyzer=\"char\",\n",
    "                ngram_range=gpu_ngram_range,\n",
    "                min_df=gpu_min_df,\n",
    "                max_features=gpu_max_features,\n",
    "            )\n",
    "\n",
    "            X_train_tfidf = tfidf_gpu.fit_transform(X_train_gpu)\n",
    "            X_val_tfidf = tfidf_gpu.transform(X_val_gpu)\n",
    "\n",
    "            clf = MBSGDClassifier(\n",
    "                loss=\"log\",\n",
    "                penalty=\"l2\",\n",
    "                alpha=1e-4,\n",
    "                epochs=2000,\n",
    "                tol=1e-3,\n",
    "                learning_rate=\"adaptive\",\n",
    "            )\n",
    "\n",
    "            clf.fit(X_train_tfidf, y_train_gpu)\n",
    "\n",
    "            val_probs_gpu = clf.predict_proba(X_val_tfidf)\n",
    "            val_preds = val_probs_gpu.values[:, 1].get() if hasattr(val_probs_gpu, \"values\") else val_probs_gpu[:, 1].get()\n",
    "\n",
    "            fold_auc = roc_auc_score(y_val, val_preds)\n",
    "            print(f\"[Model C] Fold {fold+1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "            oof_sgd[val_idx] = val_preds\n",
    "            sgd_models.append(clf)\n",
    "            joblib.dump(clf, fold_dir / \"best.joblib\")\n",
    "            sgd_model_paths.append(fold_dir / \"best.joblib\")\n",
    "            sgd_fold_auc.append(fold_auc)\n",
    "\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    except Exception as e:\n",
    "        print(f\"[Model C] GPU training failed, falling back to CPU TF-IDF+SGD. Error: {e}\")\n",
    "        oof_sgd = np.zeros(len(train_df))\n",
    "        sgd_models = []\n",
    "        sgd_model_paths = []\n",
    "        sgd_fold_auc = []\n",
    "        use_gpu_sgd = False\n",
    "\n",
    "if not use_gpu_sgd:\n",
    "    print(\"====================================\")\n",
    "    print(\"[Model C] Training on CPU with sklearn TF-IDF + SGDClassifier/LogReg\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    sgd_param_grid = [\n",
    "        {\"alpha\": 1e-4, \"eta0\": 0.1},\n",
    "        {\"alpha\": 3e-4, \"eta0\": 0.05},\n",
    "        {\"alpha\": 1e-5, \"eta0\": 0.2},\n",
    "    ]\n",
    "\n",
    "    def train_cpu_fold(fold, train_idx, val_idx):\n",
    "        print(f\"[Model C-CPU] Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "        X_train_cpu = train_df.iloc[train_idx][text_col].astype(str)\n",
    "        X_val_cpu = train_df.iloc[val_idx][text_col].astype(str)\n",
    "        if cpu_trim_chars is not None:\n",
    "            X_train_cpu = X_train_cpu.str.slice(stop=cpu_trim_chars)\n",
    "            X_val_cpu = X_val_cpu.str.slice(stop=cpu_trim_chars)\n",
    "        y_val = y[val_idx]\n",
    "\n",
    "        tfidf_cpu = TfidfVectorizer(\n",
    "            analyzer=\"char_wb\",\n",
    "            ngram_range=cpu_ngram_range,\n",
    "            min_df=cpu_min_df,\n",
    "            max_features=cpu_max_features,\n",
    "            sublinear_tf=True,\n",
    "            strip_accents=\"unicode\",\n",
    "        )\n",
    "\n",
    "        X_train_tfidf = tfidf_cpu.fit_transform(X_train_cpu)\n",
    "        X_val_tfidf = tfidf_cpu.transform(X_val_cpu)\n",
    "\n",
    "        best_model = None\n",
    "        best_auc = -np.inf\n",
    "        best_preds = None\n",
    "\n",
    "        if use_logreg_tfidf:\n",
    "            clf = LogisticRegression(\n",
    "                max_iter=800,\n",
    "                n_jobs=-1,\n",
    "                C=1.0,\n",
    "                solver=\"lbfgs\",\n",
    "                class_weight=\"balanced\",\n",
    "            )\n",
    "            clf.fit(X_train_tfidf, y[train_idx])\n",
    "            val_preds = clf.predict_proba(X_val_tfidf)[:, 1]\n",
    "            best_model, best_preds = clf, val_preds\n",
    "            best_auc = roc_auc_score(y_val, val_preds)\n",
    "        else:\n",
    "            for params in sgd_param_grid:\n",
    "                clf = SGDClassifier(\n",
    "                    loss=\"log_loss\",\n",
    "                    penalty=\"l2\",\n",
    "                    alpha=params[\"alpha\"],\n",
    "                    max_iter=2000,\n",
    "                    tol=1e-3,\n",
    "                    random_state=SEED + fold,\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    eta0=params[\"eta0\"],\n",
    "                    class_weight=\"balanced\",\n",
    "                )\n",
    "\n",
    "                clf.fit(X_train_tfidf, y[train_idx])\n",
    "                val_preds = clf.predict_proba(X_val_tfidf)[:, 1]\n",
    "                fold_auc = roc_auc_score(y_val, val_preds)\n",
    "                if fold_auc > best_auc:\n",
    "                    best_auc = fold_auc\n",
    "                    best_model = clf\n",
    "                    best_preds = val_preds\n",
    "\n",
    "        fold_auc = roc_auc_score(y_val, best_preds)\n",
    "        print(f\"[Model C-CPU] Fold {fold+1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "        fold_dir = SGD_MODEL_DIR / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(best_model, fold_dir / \"best.joblib\")\n",
    "\n",
    "        return {\n",
    "            \"fold\": fold,\n",
    "            \"val_idx\": val_idx,\n",
    "            \"val_preds\": best_preds,\n",
    "            \"clf\": best_model,\n",
    "            \"fold_auc\": fold_auc,\n",
    "            \"model_path\": fold_dir / \"best.joblib\",\n",
    "        }\n",
    "\n",
    "    cpu_results = joblib.Parallel(n_jobs=cpu_n_jobs, backend=\"loky\")(\n",
    "        joblib.delayed(train_cpu_fold)(fold, train_idx, val_idx)\n",
    "        for fold, (train_idx, val_idx) in enumerate(folds)\n",
    "    )\n",
    "\n",
    "    for res in sorted(cpu_results, key=lambda r: r[\"fold\"]):\n",
    "        oof_sgd[res[\"val_idx\"]] = res[\"val_preds\"]\n",
    "        sgd_models.append(res[\"clf\"])\n",
    "        sgd_model_paths.append(res[\"model_path\"])\n",
    "        sgd_fold_auc.append(res[\"fold_auc\"])\n",
    "\n",
    "sgd_oof_auc = roc_auc_score(y, oof_sgd)\n",
    "print(f\"Model C OOF ROC-AUC: {sgd_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_sgd\": oof_sgd}).to_csv(OOF_DIR / \"oof_sgd.csv\", index=False)\n",
    "fold_ids = list(range(len(sgd_fold_auc)))\n",
    "fold_auc_values = list(sgd_fold_auc)\n",
    "if len(fold_auc_values) != N_SPLITS:\n",
    "    print(f\"[Model C] Warning: expected {N_SPLITS} fold AUCs, got {len(fold_auc_values)}; padding with NaN\")\n",
    "    for missing_fold in range(len(fold_auc_values), N_SPLITS):\n",
    "        fold_ids.append(missing_fold)\n",
    "        fold_auc_values.append(np.nan)\n",
    "\n",
    "sgd_folds_df = pd.DataFrame({\"fold\": fold_ids, \"fold_auc\": fold_auc_values})\n",
    "sgd_folds_df.to_csv(OOF_DIR / \"oof_sgd_folds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d88061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stacked Ensemble: Logistic Regression meta-learner on OOF probabilities\n",
    "# Use cached OOF predictions so this cell can run without retraining base models.\n",
    "def load_oof_preds(name):\n",
    "    existing = globals().get(f\"oof_{name}\")\n",
    "    if existing is not None and len(existing):\n",
    "        return np.asarray(existing)\n",
    "    path = OOF_DIR / f\"oof_{name}.csv\"\n",
    "    if path.exists():\n",
    "        col = f\"oof_{name}\"\n",
    "        df = pd.read_csv(path)\n",
    "        if col in df:\n",
    "            print(f\"[Meta] Loaded {col} from {path}\")\n",
    "            return df[col].values\n",
    "    return None\n",
    "\n",
    "oof_sources = {}\n",
    "missing_oof = []\n",
    "length_mismatch = []\n",
    "for key in STACK_BASE_MODELS:\n",
    "    preds = load_oof_preds(key)\n",
    "    if preds is None:\n",
    "        missing_oof.append(key)\n",
    "        continue\n",
    "    if len(preds) != len(y):\n",
    "        length_mismatch.append((key, len(preds)))\n",
    "        continue\n",
    "    oof_sources[key] = preds\n",
    "\n",
    "if missing_oof:\n",
    "    missing_msg = \", \".join(missing_oof)\n",
    "    raise RuntimeError(\n",
    "        f\"Missing OOF predictions for: {missing_msg}. \"\n",
    "        \"Run the corresponding training cells once to cache them to disk.\"\n",
    "    )\n",
    "\n",
    "if length_mismatch:\n",
    "    mismatch_msg = \"; \".join(f\"{k} has {n} rows\" for k, n in length_mismatch)\n",
    "    raise RuntimeError(f\"OOF size mismatch ({len(y)} rows expected): {mismatch_msg}\")\n",
    "\n",
    "stack_train = np.column_stack([oof_sources[k] for k in STACK_BASE_MODELS])\n",
    "\n",
    "# Cross-validated meta learner to reduce overfitting\n",
    "meta_candidates = [0.01, 0.1, 1.0, 10]\n",
    "meta_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "meta_learner = LogisticRegressionCV(\n",
    "    Cs=meta_candidates,\n",
    "    cv=meta_skf,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    solver=\"lbfgs\",\n",
    "    scoring=\"roc_auc\",\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "meta_learner.fit(stack_train, y)\n",
    "meta_learner.base_model_order = STACK_BASE_MODELS\n",
    "best_c = float(np.ravel(meta_learner.C_)[0])\n",
    "stack_auc = roc_auc_score(y, meta_learner.predict_proba(stack_train)[:, 1])\n",
    "print(f\"Meta-learner OOF ROC-AUC: {stack_auc:.5f} (best C={best_c})\")\n",
    "\n",
    "stack_model_path = STACK_MODEL_DIR / \"meta_learner.joblib\"\n",
    "joblib.dump(meta_learner, stack_model_path)\n",
    "\n",
    "stack_oof = {f\"oof_{k}\": oof_sources[k] for k in STACK_BASE_MODELS}\n",
    "stack_oof[\"oof_stack\"] = meta_learner.predict_proba(stack_train)[:, 1]\n",
    "stack_oof[label_col] = y\n",
    "pd.DataFrame(stack_oof).to_csv(OOF_DIR / \"oof_stack.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e687489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test data (averaging fold predictions for each base model)\n",
    "def _dedupe_paths(paths):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if p.exists() and p not in seen:\n",
    "            seen.add(p)\n",
    "            unique.append(p)\n",
    "    return unique\n",
    "def predict_deberta(df):\n",
    "    tok = globals().get(\"tokenizer\") or AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "    coll = globals().get(\"collator\") or DataCollatorWithPadding(tokenizer=tok, padding=True)\n",
    "    globals()[\"tokenizer\"] = tok\n",
    "    globals()[\"collator\"] = coll\n",
    "    test_ds = HFTextDataset(df, tok, text_col, None, MAX_LENGTH)\n",
    "    fold_paths = _dedupe_paths(\n",
    "        [MODEL_DIR / f\"fold_{f}\" / \"best\" for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"deberta_model_paths\", []))\n",
    "    )\n",
    "    if not fold_paths:\n",
    "        raise RuntimeError(\"No DeBERTa checkpoints found; run Model A training first.\")\n",
    "    fold_preds = []\n",
    "    for path in fold_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "        infer_trainer = Trainer(model=model, tokenizer=tok, data_collator=coll)\n",
    "        preds = infer_trainer.predict(test_ds).predictions\n",
    "        fold_preds.append(softmax_logits(preds))\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def predict_lgb(df):\n",
    "    feats = df[num_cols]\n",
    "    lgb_dirs = [LGB_MODEL_DIR]\n",
    "    if \"LGB_MODEL_DIR_LEGACY\" in globals():\n",
    "        lgb_dirs.append(LGB_MODEL_DIR_LEGACY)\n",
    "\n",
    "    candidate_paths = _dedupe_paths(\n",
    "        [d / f\"fold_{f}\" / \"best.txt\" for d in lgb_dirs for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"lgb_model_paths\", []))\n",
    "    )\n",
    "    if candidate_paths:\n",
    "        models = [lgb.Booster(model_file=str(p)) for p in candidate_paths]\n",
    "    else:\n",
    "        models = globals().get(\"lgb_models\")\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No LightGBM models found; run Model B training first.\")\n",
    "\n",
    "    fold_preds = []\n",
    "    for m in models:\n",
    "        if isinstance(m, lgb.Booster):\n",
    "            fold_preds.append(m.predict(feats))\n",
    "        else:\n",
    "            fold_preds.append(m.predict_proba(feats, num_iteration=getattr(m, \"best_iteration_\", None))[:, 1])\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def predict_sgd(df):\n",
    "    texts = df[text_col].astype(str)\n",
    "    candidate_paths = _dedupe_paths(\n",
    "        [SGD_MODEL_DIR / f\"fold_{f}\" / \"best.joblib\" for f in range(N_SPLITS)]\n",
    "        + list(globals().get(\"sgd_model_paths\", []))\n",
    "    )\n",
    "    if candidate_paths:\n",
    "        models = [joblib.load(p) for p in candidate_paths]\n",
    "    else:\n",
    "        models = globals().get(\"sgd_models\")\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No TF-IDF+SGD models found; run Model C training first.\")\n",
    "    fold_preds = [m.predict_proba(texts)[:, 1] for m in models]\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "def load_meta_model():\n",
    "    saved_path = STACK_MODEL_DIR / \"meta_learner.joblib\"\n",
    "    if saved_path.exists():\n",
    "        return joblib.load(saved_path)\n",
    "    fallback = globals().get(\"meta_learner\")\n",
    "    if fallback is None:\n",
    "        raise RuntimeError(\"Meta-learner not trained yet; run the stacking cell.\")\n",
    "    return fallback\n",
    "if test_df is not None:\n",
    "    print(\"Running inference on test set...\")\n",
    "    base_preds = {\n",
    "        \"deberta\": predict_deberta(test_df),\n",
    "        \"lgb\": predict_lgb(test_df),\n",
    "        \"sgd\": predict_sgd(test_df),\n",
    "    }\n",
    "    meta_for_inference = load_meta_model()\n",
    "    base_order = getattr(meta_for_inference, \"base_model_order\", STACK_BASE_MODELS)\n",
    "    stack_test = np.column_stack([base_preds[name] for name in base_order])\n",
    "    test_pred = meta_for_inference.predict_proba(stack_test)[:, 1]\n",
    "    submission = pd.DataFrame({\"id\": test_df.index, \"prediction\": test_pred})\n",
    "    submission.to_csv(WORK_DIR / \"submission.csv\", index=False)\n",
    "    print(\"Saved submission.csv\")\n",
    "else:\n",
    "    print(\"No test file found; set TEST_PATH to run inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostics summary (OOF and fold-level)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Helper to grab in-memory arrays first, otherwise fall back to saved OOF files\n",
    "\n",
    "def load_oof(name):\n",
    "    in_memory = globals().get(f\"oof_{name}\")\n",
    "    if in_memory is not None:\n",
    "        return np.asarray(in_memory)\n",
    "    path = OOF_DIR / f\"oof_{name}.csv\"\n",
    "    if path.exists():\n",
    "        col = f\"oof_{name}\"\n",
    "        df = pd.read_csv(path)\n",
    "        if col in df:\n",
    "            return df[col].values\n",
    "    return None\n",
    "\n",
    "\n",
    "def fold_scores(preds):\n",
    "    scores = []\n",
    "    for fold, (_, val_idx) in enumerate(folds):\n",
    "        scores.append(roc_auc_score(y[val_idx], preds[val_idx]))\n",
    "    return scores\n",
    "\n",
    "\n",
    "summary_rows = []\n",
    "for label, key in [\n",
    "    (\"Model A: DeBERTa-v3\", \"deberta\"),\n",
    "    (\"Model B: LightGBM numeric\", \"lgb\"),\n",
    "    (\"Model C: TF-IDF + SGD\", \"sgd\"),\n",
    "    (\"Meta-learner (stack)\", \"stack\"),\n",
    "]:\n",
    "    preds = load_oof(key)\n",
    "    if preds is None:\n",
    "        print(f\"Skipping {label}: no OOF predictions found\")\n",
    "        continue\n",
    "    overall = roc_auc_score(y, preds)\n",
    "    folds_auc = fold_scores(preds) if len(preds) == len(y) else None\n",
    "    summary_rows.append(\n",
    "        OrderedDict(\n",
    "            model=label,\n",
    "            overall_auc=overall,\n",
    "            fold_mean=np.mean(folds_auc) if folds_auc else None,\n",
    "            fold_std=np.std(folds_auc) if folds_auc else None,\n",
    "            min_fold=np.min(folds_auc) if folds_auc else None,\n",
    "            max_fold=np.max(folds_auc) if folds_auc else None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "if not summary_df.empty:\n",
    "    display(summary_df.sort_values(\"overall_auc\", ascending=False))\n",
    "else:\n",
    "    print(\"No OOF data available to summarize.\")\n",
    "\n",
    "# Meta-learner diagnostics (if saved/loaded)\n",
    "meta_model = None\n",
    "if 'load_meta_model' in globals():\n",
    "    try:\n",
    "        meta_model = load_meta_model()\n",
    "    except Exception:\n",
    "        meta_model = globals().get('meta_learner')\n",
    "elif 'meta_learner' in globals():\n",
    "    meta_model = meta_learner\n",
    "\n",
    "if meta_model is not None and hasattr(meta_model, 'coef_'):\n",
    "    coef = meta_model.coef_.ravel()\n",
    "    bases = ['deberta', 'lgb', 'sgd'][: len(coef)]\n",
    "    print(\"Meta-learner coefficients (positive -> higher AI probability):\")\n",
    "    display(pd.Series(coef, index=bases))\n",
    "    if hasattr(meta_model, 'C'):\n",
    "        print(f\"Meta-learner C: {getattr(meta_model, 'C', None)}\")\n",
    "\n",
    "# Correlation between base model OOF predictions (helps assess diversity)\n",
    "base_preds = {\n",
    "    name: load_oof(name)\n",
    "    for name in ['deberta', 'lgb', 'sgd']\n",
    "}\n",
    "if all(v is not None for v in base_preds.values()):\n",
    "    corr_df = pd.DataFrame(base_preds)\n",
    "    print(\"Correlation of base model OOF predictions:\")\n",
    "    display(corr_df.corr())\n",
    "\n",
    "# Class balance reminder\n",
    "pos_rate = y.mean()\n",
    "print(f\"Positive rate in training: {pos_rate:.4f} (n={len(y)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
