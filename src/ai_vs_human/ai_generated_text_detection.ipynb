{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Generated Text Detection\n",
    "\n",
    "Stacked ensemble (DeBERTa-v3 + LightGBM + TF-IDF/SGD) on `merged_ai_human_multisocial_features.csv` (0=Human, 1=AI). Flow: setup -> data check -> Model A/B/C CV (OOF) -> stacking -> inference. Text column: `text`; label column: `label`; numerical features like `burstiness`, `perplexity_score`, `lexical_diversity`, `gunning_fog_index`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2305080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies if running in a fresh environment\n",
    "# !pip install -q pandas numpy torch torchvision torchaudio transformers datasets lightgbm scikit-learn\n",
    "# For CUDA builds of PyTorch: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import lightgbm as lgb\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "BASE_MODEL = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH = 256\n",
    "N_SPLITS = 5\n",
    "\n",
    "CWD = Path.cwd()\n",
    "candidate_paths = [\n",
    "    CWD / \"merged_ai_human_multisocial_features.csv\",\n",
    "    CWD / \"src/ai_vs_human/merged_ai_human_multisocial_features.csv\",\n",
    "]\n",
    "for cp in candidate_paths:\n",
    "    if cp.exists():\n",
    "        DATA_PATH = cp\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"merged_ai_human_multisocial_features.csv not found\")\n",
    "\n",
    "WORK_DIR = DATA_PATH.parent\n",
    "MODEL_DIR = WORK_DIR / \"models/deberta_v3_base\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR = WORK_DIR / \"oof\"\n",
    "OOF_DIR.mkdir(exist_ok=True)\n",
    "test_candidates = [\n",
    "    WORK_DIR / \"merged_ai_human_multisocial_features_test.csv\",\n",
    "    WORK_DIR / \"ai_human_content_detection_test.csv\",\n",
    "]\n",
    "TEST_PATH = next((p for p in test_candidates if p.exists()), test_candidates[0])  # optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fa7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and inspect\n",
    "train_df = pd.read_csv(DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH) if TEST_PATH.exists() else None\n",
    "\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "meta_prefixes = (\"src_\",)  # exclude metadata to avoid leakage\n",
    "\n",
    "num_cols = [\n",
    "    c\n",
    "    for c in train_df.columns\n",
    "    if c not in [text_col, label_col]\n",
    "    and pd.api.types.is_numeric_dtype(train_df[c])\n",
    "    and not any(c.startswith(pref) for pref in meta_prefixes)\n",
    "]\n",
    "\n",
    "train_df[num_cols] = train_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "num_medians = train_df[num_cols].median()\n",
    "train_df[num_cols] = train_df[num_cols].fillna(num_medians)\n",
    "if test_df is not None:\n",
    "    test_df[num_cols] = test_df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(num_medians)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Numeric features ({len(num_cols)}): {num_cols[:10]}{'...' if len(num_cols) > 10 else ''}\")\n",
    "print(train_df[[text_col, label_col]].head(2))\n",
    "print(train_df[num_cols].describe().T.head())\n",
    "\n",
    "y = train_df[label_col].astype(int).values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "folds = list(skf.split(train_df[text_col], y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c62fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper classes and metrics\n",
    "class HFTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, text_col, label_col=None, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.df.loc[idx, self.text_col])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "        if self.label_col is not None:\n",
    "            enc[\"labels\"] = int(self.df.loc[idx, self.label_col])\n",
    "        return enc\n",
    "\n",
    "\n",
    "def softmax_logits(logits):\n",
    "    logits = torch.tensor(logits)\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs[:, 1]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = softmax_logits(logits)\n",
    "    return {\"roc_auc\": roc_auc_score(labels, probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1836/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1836/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1836/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1836/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Model A] Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1836/364608256.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "/home/ruben/EC/.venv/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/36 00:12 < 00:33, 0.74 it/s, Epoch 0.57/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model A: DeBERTa-v3-base with Stratified K-Fold OOF\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "oof_deberta = np.zeros(len(train_df))\n",
    "deberta_model_paths = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n[Model A] Fold {fold+1}/{N_SPLITS}\")\n",
    "    fold_dir = MODEL_DIR / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ds = HFTextDataset(train_df.iloc[train_idx], tokenizer, text_col, label_col, MAX_LENGTH)\n",
    "    val_ds = HFTextDataset(train_df.iloc[val_idx], tokenizer, text_col, label_col, MAX_LENGTH)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        BASE_MODEL, num_labels=2\n",
    "    )\n",
    "\n",
    "    training_kwargs = dict(\n",
    "        output_dir=str(fold_dir),\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"roc_auc\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_num_workers=2,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Handle TrainingArguments API differences across transformers versions\n",
    "    try:\n",
    "        args = TrainingArguments(\n",
    "            **training_kwargs,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "        )\n",
    "    except TypeError:\n",
    "        try:\n",
    "            args = TrainingArguments(\n",
    "                **training_kwargs,\n",
    "                evaluate_during_training=True,\n",
    "                eval_steps=500,\n",
    "                save_steps=500,\n",
    "            )\n",
    "        except TypeError:\n",
    "            fallback_kwargs = training_kwargs.copy()\n",
    "            for key in (\"load_best_model_at_end\", \"metric_for_best_model\", \"greater_is_better\"):\n",
    "                fallback_kwargs.pop(key, None)\n",
    "            args = TrainingArguments(**fallback_kwargs)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    preds = trainer.predict(val_ds).predictions\n",
    "    oof_deberta[val_idx] = softmax_logits(preds)\n",
    "\n",
    "    best_dir = trainer.state.best_model_checkpoint or str(fold_dir / \"best\")\n",
    "    if trainer.state.best_model_checkpoint is None:\n",
    "        trainer.save_model(best_dir)\n",
    "    deberta_model_paths.append(best_dir)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "deberta_oof_auc = roc_auc_score(y, oof_deberta)\n",
    "print(f\"Model A OOF ROC-AUC: {deberta_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_deberta\": oof_deberta}).to_csv(OOF_DIR / \"oof_deberta.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model B: LightGBM on numerical features only\n",
    "oof_lgb = np.zeros(len(train_df))\n",
    "lgb_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n[Model B] Fold {fold+1}/{N_SPLITS}\")\n",
    "    X_train = train_df.iloc[train_idx][num_cols]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = train_df.iloc[val_idx][num_cols]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=128,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary\",\n",
    "        random_state=SEED + fold,\n",
    "        n_jobs=-1,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_samples=40,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"auc\",\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False)],\n",
    "    )\n",
    "\n",
    "    oof_lgb[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    lgb_models.append(model)\n",
    "\n",
    "lgb_oof_auc = roc_auc_score(y, oof_lgb)\n",
    "print(f\"Model B OOF ROC-AUC: {lgb_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_lgb\": oof_lgb}).to_csv(OOF_DIR / \"oof_lgb.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model C: TF-IDF (3-5 char-grams) + SGDClassifier(log_loss)\n",
    "oof_sgd = np.zeros(len(train_df))\n",
    "sgd_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    print(f\"\\n[Model C] Fold {fold+1}/{N_SPLITS}\")\n",
    "    X_train = train_df.iloc[train_idx][text_col].astype(str)\n",
    "    y_train = y[train_idx]\n",
    "    X_val = train_df.iloc[val_idx][text_col].astype(str)\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"tfidf\",\n",
    "                TfidfVectorizer(\n",
    "                    analyzer=\"char\",\n",
    "                    ngram_range=(3, 5),\n",
    "                    sublinear_tf=True,\n",
    "                    min_df=2,\n",
    "                    max_features=600000,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"clf\",\n",
    "                SGDClassifier(\n",
    "                    loss=\"log_loss\",\n",
    "                    alpha=1e-4,\n",
    "                    max_iter=1000,\n",
    "                    tol=1e-3,\n",
    "                    random_state=SEED + fold,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    oof_sgd[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
    "    sgd_models.append(clf)\n",
    "\n",
    "sgd_oof_auc = roc_auc_score(y, oof_sgd)\n",
    "print(f\"Model C OOF ROC-AUC: {sgd_oof_auc:.5f}\")\n",
    "pd.DataFrame({\"oof_sgd\": oof_sgd}).to_csv(OOF_DIR / \"oof_sgd.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Ensemble: Logistic Regression meta-learner on OOF probabilities\n",
    "stack_train = np.vstack([oof_deberta, oof_lgb, oof_sgd]).T\n",
    "\n",
    "meta_learner = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    C=2.0,\n",
    "    solver=\"lbfgs\",\n",
    ")\n",
    "meta_learner.fit(stack_train, y)\n",
    "stack_auc = roc_auc_score(y, meta_learner.predict_proba(stack_train)[:, 1])\n",
    "print(f\"Meta-learner OOF ROC-AUC: {stack_auc:.5f}\")\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"oof_deberta\": oof_deberta,\n",
    "        \"oof_lgb\": oof_lgb,\n",
    "        \"oof_sgd\": oof_sgd,\n",
    "        \"oof_stack\": meta_learner.predict_proba(stack_train)[:, 1],\n",
    "        label_col: y,\n",
    "    }\n",
    ").to_csv(OOF_DIR / \"oof_stack.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test data (averaging fold predictions for each base model)\n",
    "def predict_deberta(df):\n",
    "    test_ds = HFTextDataset(df, tokenizer, text_col, None, MAX_LENGTH)\n",
    "    fold_preds = []\n",
    "    for path in deberta_model_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "        infer_trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collator,\n",
    "        )\n",
    "        preds = infer_trainer.predict(test_ds).predictions\n",
    "        fold_preds.append(softmax_logits(preds))\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "\n",
    "\n",
    "def predict_lgb(df):\n",
    "    feats = df[num_cols]\n",
    "    fold_preds = [m.predict_proba(feats)[:, 1] for m in lgb_models]\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "\n",
    "\n",
    "def predict_sgd(df):\n",
    "    texts = df[text_col].astype(str)\n",
    "    fold_preds = [m.predict_proba(texts)[:, 1] for m in sgd_models]\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "\n",
    "\n",
    "if test_df is not None:\n",
    "    print(\"\\nRunning inference on test set...\")\n",
    "    test_deberta = predict_deberta(test_df)\n",
    "    test_lgb = predict_lgb(test_df)\n",
    "    test_sgd = predict_sgd(test_df)\n",
    "\n",
    "    stack_test = np.vstack([test_deberta, test_lgb, test_sgd]).T\n",
    "    test_pred = meta_learner.predict_proba(stack_test)[:, 1]\n",
    "\n",
    "    submission = pd.DataFrame({\"id\": test_df.index, \"prediction\": test_pred})\n",
    "    submission.to_csv(WORK_DIR / \"submission.csv\", index=False)\n",
    "    print(\"Saved submission.csv\")\n",
    "else:\n",
    "    print(\"No test file found; set TEST_PATH to run inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
