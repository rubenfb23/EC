{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3a028e",
   "metadata": {},
   "source": [
    "# AI_Human + MultiSocial dataset builder\n",
    "\n",
    "Merge AI_Human.csv and multisocial_anonymized.csv, clean the text, and generate a feature rich table ready for ML training. The flow mirrors the feature engineering used in ai_vs_human_AI_Human.ipynb and adds metadata signals (language, source, model id, potential_noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc39e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ruben/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import textstat\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"src/ai_vs_human\")\n",
    "AI_HUMAN_PATH = \"AI_Human.csv\"\n",
    "MULTI_PATH = \"multisocial_anonymized.csv\"\n",
    "RAW_OUTPUT = \"merged_raw_clean.csv\"\n",
    "FEATURE_OUTPUT = \"merged_ai_human_multisocial_features.csv\"\n",
    "FEATURE_CACHE = \"cache_merged_text_features.csv\"\n",
    "# Controls\n",
    "RANDOM_STATE = 42\n",
    "DROP_POTENTIAL_NOISE = True\n",
    "SAMPLE_ROWS = None  # set to an int (e.g., 200000) for quick dry runs\n",
    "NORMALIZE_TEXT = True\n",
    "USE_FEATURE_CACHE = True\n",
    "TOP_LANGS = 12\n",
    "TOP_SOURCES = 15\n",
    "TOP_MODELS = 20\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9ede0",
   "metadata": {},
   "source": [
    "## 1. Load and harmonize datasets\n",
    "- Normalize column names\n",
    "- Keep a shared schema (text, label, language, source, model id, potential_noise, length, dataset)\n",
    "- Optionally drop noisy rows flagged by MultiSocial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e58a820",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/ai_vs_human/AI_Human.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# AI_Human\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ai_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAI_HUMAN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m ai_df = ai_df.rename(columns={\u001b[33m\"\u001b[39m\u001b[33mgenerated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      4\u001b[39m ai_df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] = ai_df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].fillna(\u001b[32m0\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/EC/.venv/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'src/ai_vs_human/AI_Human.csv'"
     ]
    }
   ],
   "source": [
    "# AI_Human\n",
    "ai_df = pd.read_csv(AI_HUMAN_PATH)\n",
    "ai_df = ai_df.rename(columns={\"generated\": \"label\"})\n",
    "ai_df[\"label\"] = ai_df[\"label\"].fillna(0).astype(int)\n",
    "ai_df[\"text\"] = ai_df[\"text\"].fillna(\"\").astype(str)\n",
    "ai_df[\"dataset\"] = \"ai_human\"\n",
    "ai_df[\"language\"] = \"unknown\"\n",
    "ai_df[\"source\"] = \"ai_human\"\n",
    "ai_df[\"multi_label\"] = np.where(ai_df[\"label\"] == 1, \"unknown_model\", \"human\")\n",
    "ai_df[\"potential_noise\"] = 0\n",
    "ai_df[\"split\"] = \"unspecified\"\n",
    "ai_df[\"length\"] = ai_df[\"text\"].str.len()\n",
    "\n",
    "# MultiSocial\n",
    "multi_df = pd.read_csv(MULTI_PATH)\n",
    "multi_df[\"text\"] = multi_df[\"text\"].fillna(\"\").astype(str)\n",
    "multi_df[\"label\"] = multi_df[\"label\"].astype(int)\n",
    "multi_df[\"dataset\"] = \"multisocial\"\n",
    "multi_df[\"multi_label\"] = multi_df[\"multi_label\"].fillna(\"unknown_model\")\n",
    "\n",
    "if DROP_POTENTIAL_NOISE and \"potential_noise\" in multi_df.columns:\n",
    "    before = len(multi_df)\n",
    "    multi_df = multi_df[multi_df[\"potential_noise\"] == 0].copy()\n",
    "    print(f\"Dropped potential_noise rows: {before - len(multi_df)}\")\n",
    "\n",
    "# Ensure required columns exist everywhere\n",
    "required_defaults = {\n",
    "    \"language\": \"unknown\",\n",
    "    \"source\": \"unknown\",\n",
    "    \"multi_label\": \"unknown_model\",\n",
    "    \"potential_noise\": 0,\n",
    "    \"split\": \"unspecified\",\n",
    "    \"length\": None,\n",
    "}\n",
    "for col, default in required_defaults.items():\n",
    "    for df in (ai_df, multi_df):\n",
    "        if col not in df.columns:\n",
    "            df[col] = default\n",
    "\n",
    "combined = pd.concat([ai_df, multi_df], ignore_index=True)\n",
    "print(f\"Combined raw shape: {combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6070a",
   "metadata": {},
   "source": [
    "## 2. Clean text and basic checks\n",
    "- Strip whitespace and collapse multiple spaces\n",
    "- Remove rows with empty text\n",
    "- Drop duplicates by (text, label)\n",
    "- Optional sampling for quick experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ae1eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 24) (581094703.py, line 24)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"Label distribution:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 24)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "if NORMALIZE_TEXT:\n",
    "    combined[\"text\"] = combined[\"text\"].apply(clean_text)\n",
    "else:\n",
    "    combined[\"text\"] = combined[\"text\"].astype(str)\n",
    "\n",
    "# Remove empties and recompute length after cleaning\n",
    "combined = combined[combined[\"text\"].str.len() > 0].copy()\n",
    "combined[\"length\"] = combined[\"text\"].str.len()\n",
    "\n",
    "before_dup = len(combined)\n",
    "combined = combined.drop_duplicates(subset=[\"text\", \"label\"])\n",
    "print(f\"Dropped duplicates: {before_dup - len(combined)}\")\n",
    "\n",
    "if SAMPLE_ROWS:\n",
    "    combined = combined.sample(n=min(SAMPLE_ROWS, len(combined)), random_state=RANDOM_STATE)\n",
    "\n",
    "combined = combined.reset_index(drop=True)\n",
    "print(f\"Merged and cleaned shape: {combined.shape}\")\n",
    "print(\"Label distribution:\", combined[\"label\"].value_counts())\n",
    "print(\"Rows by dataset:\", combined[\"dataset\"].value_counts())\n",
    "print(\"Top languages:\", combined[\"language\"].value_counts().head(10))\n",
    "display(combined.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c805765",
   "metadata": {},
   "source": [
    "## 3. Helper to cap high-cardinality categorical features\n",
    "Keeps only the top-N most frequent categories and buckets the rest into `other`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_categories(series: pd.Series, top_k: int, other_label: str = \"other\") -> pd.Series:\n",
    "    top = series.value_counts().nlargest(top_k).index\n",
    "    return series.where(series.isin(top), other_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75c71a",
   "metadata": {},
   "source": [
    "## 4. Feature engineering on text (same core block as ai_vs_human_AI_Human)\n",
    "Creates readability, lexical diversity, burstiness, sentiment, punctuation ratio, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "punct_re = re.compile(r'[^\\w\\s]')\n",
    "passive_re = re.compile(r'\\b(?:is|are|was|were|be|been|being)\\s+\\w+ed\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def safe_readability(fn, text: str):\n",
    "    try:\n",
    "        return float(fn(text))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def extract_features(text: str) -> pd.Series:\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    clean = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', clean) if s.strip()]\n",
    "    sentence_count = max(len(sentences), 1)\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', clean.lower())\n",
    "    word_count = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    lexical_diversity = unique_words / word_count if word_count else 0.0\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0.0\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0.0\n",
    "    character_count = len(clean)\n",
    "    punctuation_ratio = len(punct_re.findall(clean)) / character_count if character_count else 0.0\n",
    "\n",
    "    fre = safe_readability(textstat.flesch_reading_ease, clean) if word_count else 0.0\n",
    "    fog = safe_readability(textstat.gunning_fog, clean) if word_count else 0.0\n",
    "\n",
    "    sentence_lengths = [len(s.split()) for s in sentences] or [0]\n",
    "    burstiness = np.std(sentence_lengths) / (np.mean(sentence_lengths) + 1e-6)\n",
    "\n",
    "    sentiment_score = sia.polarity_scores(clean)['compound'] if word_count else 0.0\n",
    "    passive_voice_ratio = len(passive_re.findall(clean)) / sentence_count\n",
    "\n",
    "    predictability_score = 1 - lexical_diversity if word_count else 0.0\n",
    "    grammar_errors = len(re.findall(r\"\\b(?:ain't|could of|should of|would of)\\b\", clean.lower()))\n",
    "\n",
    "    return pd.Series({\n",
    "        'word_count': word_count,\n",
    "        'character_count': character_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'flesch_reading_ease': fre,\n",
    "        'gunning_fog_index': fog,\n",
    "        'grammar_errors': grammar_errors,\n",
    "        'passive_voice_ratio': passive_voice_ratio,\n",
    "        'predictability_score': predictability_score,\n",
    "        'burstiness': burstiness,\n",
    "        'sentiment_score': sentiment_score,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51773856",
   "metadata": {},
   "source": [
    "## 5. Generate text feature matrix (with optional cache)\n",
    "`FEATURE_CACHE` stores only the text-derived features to skip recomputation when the cleaned dataset length is unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "text_features = None\n",
    "\n",
    "# Ensure cache path is Path even if the variable was a string in a prior run\n",
    "FEATURE_CACHE = Path(FEATURE_CACHE)\n",
    "\n",
    "if USE_FEATURE_CACHE and FEATURE_CACHE.exists():\n",
    "    cached = pd.read_csv(FEATURE_CACHE)\n",
    "    if len(cached) == len(combined):\n",
    "        text_features = cached\n",
    "        print(f\"Loaded cached text features from {FEATURE_CACHE}\")\n",
    "    else:\n",
    "        print(\"Cache size mismatch, recalculating...\")\n",
    "\n",
    "if text_features is None:\n",
    "    print(\"Building text features...\")\n",
    "    text_features = combined[\"text\"].progress_apply(extract_features)\n",
    "    if USE_FEATURE_CACHE:\n",
    "        try:\n",
    "            text_features.to_csv(FEATURE_CACHE, index=False)\n",
    "            print(f\"Saved text feature cache to {FEATURE_CACHE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save cache: {e}\")\n",
    "\n",
    "print(\"Text features shape:\", text_features.shape)\n",
    "display(text_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413b9f6",
   "metadata": {},
   "source": [
    "## 6. Add metadata signals and build final feature table\n",
    "- Language, source, model id, dataset origin (capped to most common categories)\n",
    "- Numeric metadata: text length and potential_noise flag\n",
    "- Target column: `label`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"language_norm\"] = cap_categories(combined[\"language\"].fillna(\"unknown\"), TOP_LANGS)\n",
    "combined[\"source_norm\"] = cap_categories(combined[\"source\"].fillna(\"unknown\"), TOP_SOURCES)\n",
    "combined[\"model_norm\"] = cap_categories(combined[\"multi_label\"].fillna(\"unknown_model\"), TOP_MODELS)\n",
    "combined[\"dataset_norm\"] = combined[\"dataset\"].fillna(\"unknown\")\n",
    "\n",
    "lang_dummies = pd.get_dummies(combined[\"language_norm\"], prefix=\"lang\")\n",
    "source_dummies = pd.get_dummies(combined[\"source_norm\"], prefix=\"src\")\n",
    "model_dummies = pd.get_dummies(combined[\"model_norm\"], prefix=\"model\")\n",
    "dataset_dummies = pd.get_dummies(combined[\"dataset_norm\"], prefix=\"ds\")\n",
    "\n",
    "meta_numeric = combined[[\"length\", \"potential_noise\"]].copy()\n",
    "\n",
    "feature_blocks = [\n",
    "    text_features.reset_index(drop=True),\n",
    "    meta_numeric.reset_index(drop=True),\n",
    "    lang_dummies.reset_index(drop=True),\n",
    "    source_dummies.reset_index(drop=True),\n",
    "    model_dummies.reset_index(drop=True),\n",
    "    dataset_dummies.reset_index(drop=True),\n",
    "]\n",
    "\n",
    "final_features = pd.concat(feature_blocks, axis=1)\n",
    "final_features.insert(0, \"text\", combined[\"text\"].values)\n",
    "final_features[\"label\"] = combined[\"label\"].astype(int).values\n",
    "\n",
    "print(\"Final feature matrix:\", final_features.shape)\n",
    "display(final_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671ff48",
   "metadata": {},
   "source": [
    "## 7. Persist cleaned raw data and features\n",
    "Both CSVs keep the exact row order used for feature generation (safe to reload and align on index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a851bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_export_cols = [\n",
    "    \"text\", \"label\", \"language\", \"source\", \"multi_label\", \"potential_noise\", \"length\", \"dataset\"\n",
    "]\n",
    "combined[raw_export_cols].to_csv(RAW_OUTPUT, index=False)\n",
    "final_features.to_csv(FEATURE_OUTPUT, index=False)\n",
    "\n",
    "print(f\"Saved cleaned raw text to {RAW_OUTPUT}\")\n",
    "print(f\"Saved feature table to {FEATURE_OUTPUT}\")\n",
    "print(\"Label ratio in features:\n",
    "\", final_features[\"label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624b111",
   "metadata": {},
   "source": [
    "## 8. Quick train/validation split example\n",
    "Use the feature matrix directly with classical models (LR/XGB/RF, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef898b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_features.drop(columns=[\"label\"])\n",
    "y = final_features[\"label\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Train/val shapes:\", X_train.shape, X_val.shape)\n",
    "print(\"Class balance train:\", y_train.value_counts(normalize=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
