{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa57b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # AI Detection Research: Stress Testing Suite\n",
    "# \n",
    "# Este notebook implementa el roadmap de validación científica para elevar el nivel del detector de \"Kaggle solution\" a \"Research Paper\".\n",
    "# \n",
    "# **Hardware:** 4x NVIDIA RTX 3090  \n",
    "# **Objetivos:**\n",
    "# 1. **Robustez Adversarial:** ¿Aguanta el modelo ataques de paráfrasis?\n",
    "# 2. **Generalización (OOD):** ¿Detecta modelos nuevos (Phi-3/Llama-3) no vistos en el training?\n",
    "# 3. **Explicabilidad (XAI):** Análisis SHAP para entender qué features lingüísticas pesan más.\n",
    "\n",
    "# %% [code]\n",
    "# 1. Imports y Configuración\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline, DataCollatorWithPadding, Trainer\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "# Compat: algunas versiones de transformers no exponen `seen_tokens` en DynamicCache, requerido por Phi-3\n",
    "if not hasattr(DynamicCache, \"seen_tokens\"):\n",
    "    DynamicCache.seen_tokens = property(lambda self: self.get_seq_length())\n",
    "if not hasattr(DynamicCache, \"get_max_length\"):\n",
    "    DynamicCache.get_max_length = lambda self: getattr(self, \"max_cache_len\", None)\n",
    "\n",
    "# Configuración de Hardware\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"GPUs available: {torch.cuda.device_count()} (Will use DataParallel or device_map for heavy models)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 0. Carga de tu Modelo Entrenado\n",
    "# **IMPORTANTE:** Aquí debes instanciar tu pipeline de predicción tal como lo definiste en el notebook anterior.\n",
    "# Necesitamos una función `predict_pipeline(texts: list) -> np.array` que devuelva la probabilidad de ser IA.\n",
    "\n",
    "# %% [code]\n",
    "# --- ZONA DE INTEGRACIÓN ---\n",
    "# Cargamos el ensemble real entrenado (DeBERTa + LightGBM + meta-learner)\n",
    "print(\"✅ Cargando ensemble DeBERTa + LightGBM + meta-learner (stack)\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import textstat\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Paths y constantes (coinciden con el notebook de entrenamiento)\n",
    "WORK_DIR = Path(\"src/ai_vs_human\")\n",
    "DEBERTA_DIR = WORK_DIR / \"models\" / \"deberta_v3_base\"\n",
    "LGB_DIR = WORK_DIR / \"models\" / \"lightgbm_numeric\"\n",
    "META_PATH = WORK_DIR / \"models\" / \"stack_meta\" / \"meta_learner.joblib\"\n",
    "MAX_LENGTH = 256\n",
    "N_SPLITS = 5\n",
    "\n",
    "# Columnas exactas usadas por LightGBM en el entrenamiento\n",
    "NUM_COLS = [\n",
    "    'word_count', 'character_count', 'sentence_count', 'lexical_diversity', 'avg_sentence_length', 'avg_word_length',\n",
    "    'punctuation_ratio', 'flesch_reading_ease', 'gunning_fog_index', 'grammar_errors', 'passive_voice_ratio',\n",
    "    'predictability_score', 'burstiness', 'sentiment_score', 'length', 'potential_noise',\n",
    "    'lang_ar', 'lang_ca', 'lang_de', 'lang_en', 'lang_es', 'lang_hr', 'lang_nl', 'lang_other', 'lang_pl', 'lang_pt',\n",
    "    'lang_ro', 'lang_ru', 'lang_unknown',\n",
    "    'model_Mistral-7B-Instruct-v0.2', 'model_aya-101', 'model_gemini', 'model_gpt-3.5-turbo-0125', 'model_human',\n",
    "    'model_opt-iml-max-30b', 'model_unknown_model', 'model_v5-Eagle-7B-HF', 'model_vicuna-13b',\n",
    "    'ds_ai_human', 'ds_multisocial'\n",
    "]\n",
    "LANG_COLS = [c for c in NUM_COLS if c.startswith(\"lang_\")]\n",
    "MODEL_COLS = [c for c in NUM_COLS if c.startswith(\"model_\")]\n",
    "DS_COLS = [c for c in NUM_COLS if c.startswith(\"ds_\")]\n",
    "\n",
    "# Utilidades de features (idénticas al dataset builder)\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    import nltk\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "punct_re = re.compile(r'[^\\w\\s]')\n",
    "passive_re = re.compile(r'\\b(?:is|are|was|were|be|been|being)\\s+\\w+ed\\b', re.IGNORECASE)\n",
    "\n",
    "def safe_readability(fn, text: str):\n",
    "    try:\n",
    "        return float(fn(text))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def extract_features(text: str) -> pd.Series:\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    clean = text.replace('\\n', ' ').strip()\n",
    "\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', clean) if s.strip()]\n",
    "    sentence_count = max(len(sentences), 1)\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', clean.lower())\n",
    "    word_count = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    lexical_diversity = unique_words / word_count if word_count else 0.0\n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0.0\n",
    "    avg_word_length = np.mean([len(w) for w in words]) if words else 0.0\n",
    "    character_count = len(clean)\n",
    "    punctuation_ratio = len(punct_re.findall(clean)) / character_count if character_count else 0.0\n",
    "\n",
    "    fre = safe_readability(textstat.flesch_reading_ease, clean) if word_count else 0.0\n",
    "    fog = safe_readability(textstat.gunning_fog, clean) if word_count else 0.0\n",
    "\n",
    "    sentence_lengths = [len(s.split()) for s in sentences] or [0]\n",
    "    burstiness = np.std(sentence_lengths) / (np.mean(sentence_lengths) + 1e-6)\n",
    "\n",
    "    sentiment_score = sia.polarity_scores(clean)['compound'] if word_count else 0.0\n",
    "    passive_voice_ratio = len(passive_re.findall(clean)) / sentence_count\n",
    "\n",
    "    predictability_score = 1 - lexical_diversity if word_count else 0.0\n",
    "    grammar_errors = len(re.findall(r\"\\b(?:ain't|could of|should of|would of)\\b\", clean.lower()))\n",
    "\n",
    "    return pd.Series({\n",
    "        'word_count': word_count,\n",
    "        'character_count': character_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'avg_sentence_length': avg_sentence_length,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'punctuation_ratio': punctuation_ratio,\n",
    "        'flesch_reading_ease': fre,\n",
    "        'gunning_fog_index': fog,\n",
    "        'grammar_errors': grammar_errors,\n",
    "        'passive_voice_ratio': passive_voice_ratio,\n",
    "        'predictability_score': predictability_score,\n",
    "        'burstiness': burstiness,\n",
    "        'sentiment_score': sentiment_score,\n",
    "    })\n",
    "\n",
    "def build_feature_frame(texts):\n",
    "    rows = []\n",
    "    for t in texts:\n",
    "        feats = extract_features(t)\n",
    "        feats['length'] = len(t) if isinstance(t, str) else 0\n",
    "        feats['potential_noise'] = 0\n",
    "        for col in LANG_COLS:\n",
    "            feats[col] = 1.0 if col == 'lang_unknown' else 0.0\n",
    "        for col in MODEL_COLS:\n",
    "            feats[col] = 1.0 if col == 'model_unknown_model' else 0.0\n",
    "        for col in DS_COLS:\n",
    "            feats[col] = 0.0\n",
    "        feats['ds_ai_human'] = 1.0  # marcamos dataset desconocido como closest a ai_human\n",
    "        rows.append(feats)\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Asegurar orden/ausencia de columnas\n",
    "    for col in NUM_COLS:\n",
    "        if col not in df:\n",
    "            df[col] = 0.0\n",
    "    return df[NUM_COLS]\n",
    "\n",
    "# Dataset helper para DeBERTa\n",
    "class HFTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = list(texts)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "def load_deberta_paths():\n",
    "    paths = []\n",
    "    for f in range(N_SPLITS):\n",
    "        cand = DEBERTA_DIR / f\"fold_{f}\" / \"best\"\n",
    "        if cand.exists():\n",
    "            paths.append(cand)\n",
    "    if not paths:\n",
    "        raise RuntimeError(\"No se encontraron checkpoints de DeBERTa en models/deberta_v3_base\")\n",
    "    return paths\n",
    "\n",
    "def predict_deberta(texts):\n",
    "    fold_paths = load_deberta_paths()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(fold_paths[0], use_fast=False)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "    dataset = HFTextDataset(texts, tokenizer, max_length=MAX_LENGTH)\n",
    "    fold_preds = []\n",
    "    for path in fold_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "        trainer = Trainer(model=model, tokenizer=tokenizer, data_collator=collator)\n",
    "        logits = trainer.predict(dataset).predictions\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).cpu().numpy()[:, 1]\n",
    "        fold_preds.append(probs)\n",
    "        torch.cuda.empty_cache()\n",
    "    return np.mean(fold_preds, axis=0)\n",
    "\n",
    "def load_lgb_models():\n",
    "    models = []\n",
    "    for f in range(N_SPLITS):\n",
    "        cand = LGB_DIR / f\"fold_{f}\" / \"best.txt\"\n",
    "        if cand.exists():\n",
    "            models.append(lgb.Booster(model_file=str(cand)))\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No se encontraron modelos LightGBM en models/lightgbm_numeric\")\n",
    "    return models\n",
    "\n",
    "LGB_MODELS = load_lgb_models()\n",
    "META_MODEL = joblib.load(META_PATH)\n",
    "\n",
    "class MyEnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.base_order = getattr(META_MODEL, \"base_model_order\", [\"deberta\", \"lgb\", \"sgd\"])\n",
    "        self.lgb_models = LGB_MODELS\n",
    "        self.meta = META_MODEL\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        texts = list(texts)\n",
    "        deberta_probs = predict_deberta(texts)\n",
    "        lgb_probs = self.predict_lgb(texts)\n",
    "        # Los SGD originales guardados no incluyen vectorizador -> usamos placeholder neutral\n",
    "        sgd_probs = np.full(len(texts), 0.5)\n",
    "        base_preds = {\"deberta\": deberta_probs, \"lgb\": lgb_probs, \"sgd\": sgd_probs}\n",
    "        stack = np.column_stack([base_preds[name] for name in self.base_order])\n",
    "        return self.meta.predict_proba(stack)[:, 1]\n",
    "\n",
    "    def predict_lgb(self, texts):\n",
    "        feats = build_feature_frame(texts)\n",
    "        fold_preds = [m.predict(feats) for m in self.lgb_models]\n",
    "        return np.mean(fold_preds, axis=0)\n",
    "\n",
    "    def get_numeric_features(self, texts):\n",
    "        return build_feature_frame(texts)\n",
    "\n",
    "# Instancia tu modelo real aquí\n",
    "my_model = MyEnsembleModel()\n",
    "# ---------------------------\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Robustez Adversarial (Paraphrasing Attack)\n",
    "# Generamos versiones parafraseadas de textos que sabemos que son IA para ver si logran evadir la detección.\n",
    "\n",
    "# %% [code]\n",
    "print(\"--- Iniciando Ataque Adversarial ---\")\n",
    "\n",
    "# 1. Cargar Modelo de Paráfrasis (T5)\n",
    "# Es ligero y efectivo para cambiar estructura sin cambiar significado\n",
    "para_model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
    "para_tokenizer = AutoTokenizer.from_pretrained(para_model_name)\n",
    "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_model_name).to(DEVICE)\n",
    "\n",
    "def paraphrase_text(text, beams=5, grams=1):\n",
    "    text = \"paraphrase: \" + text + \" </s>\"\n",
    "    encoding = para_tokenizer(text, padding=\"longest\", return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = para_model.generate(\n",
    "        input_ids=encoding['input_ids'], \n",
    "        attention_mask=encoding['attention_mask'],\n",
    "        max_length=256, \n",
    "        do_sample=True, \n",
    "        top_k=120, \n",
    "        top_p=0.95, \n",
    "        early_stopping=True, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return para_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 2. Dataset de Prueba (Usar un subset de tus datos etiquetados como AI)\n",
    "# Simulación: Creamos textos dummy\n",
    "ai_texts_original = [\n",
    "    \"Artificial Intelligence has revolutionized the way we process data.\",\n",
    "    \"Deep learning models are becoming increasingly complex and accurate.\",\n",
    "    \"The rapid expansion of neural networks poses ethical questions.\"\n",
    "] * 10 # 30 ejemplos\n",
    "\n",
    "# 3. Generar Ataques\n",
    "print(f\"Generando paráfrasis para {len(ai_texts_original)} textos...\")\n",
    "ai_texts_attacked = []\n",
    "for t in tqdm(ai_texts_original):\n",
    "    try:\n",
    "        p = paraphrase_text(t)\n",
    "        ai_texts_attacked.append(p)\n",
    "    except:\n",
    "        ai_texts_attacked.append(t) # Fallback\n",
    "\n",
    "# 4. Evaluar Robustez\n",
    "preds_original = my_model.predict_proba(ai_texts_original)\n",
    "preds_attacked = my_model.predict_proba(ai_texts_attacked)\n",
    "\n",
    "# Métricas\n",
    "drop_in_confidence = np.mean(preds_original) - np.mean(preds_attacked)\n",
    "success_attack_rate = np.mean(np.array(preds_attacked) < 0.5) * 100 # % que bajó de 0.5\n",
    "\n",
    "print(f\"\\nResultados Adversariales:\")\n",
    "print(f\"Confianza Media Original: {np.mean(preds_original):.4f}\")\n",
    "print(f\"Confianza Media Atacada:  {np.mean(preds_attacked):.4f}\")\n",
    "print(f\"Caída de Confianza:       {drop_in_confidence:.4f}\")\n",
    "print(f\"Attack Success Rate (Flip to Human): {success_attack_rate:.2f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Generalización Out-of-Distribution (New GenAI Models)\n",
    "# Generamos texto con un modelo moderno (Phi-3 o Llama-3) que no estaba en el training set (normalmente GPT-3/4).\n",
    "# Aprovechamos tus 4x 3090 para esto.\n",
    "\n",
    "# %% [code]\n",
    "print(\"\\n--- Iniciando Test OOD (Phi-3/Llama-3) ---\")\n",
    "\n",
    "# Usamos Phi-3 Mini por eficiencia, o cambia a \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id_gen = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "model_id_gen = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id_gen,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\", # Usará tus múltiples GPUs automáticamente\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Desactiva la caché de generación para evitar incompatibilidades con versiones antiguas de transformers\n",
    "generator.model.config.use_cache = False\n",
    "generator.model.generation_config.use_cache = False\n",
    "\n",
    "prompts = [\n",
    "    \"Explain the theory of relativity to a 5 year old.\",\n",
    "    \"Write a polite email declining a job offer.\",\n",
    "    \"Describe a sunset on Mars.\",\n",
    "    \"Write a Python function to reverse a linked list.\",\n",
    "    \"Argue for and against universal basic income.\"\n",
    "] * 5 # 25 textos nuevos\n",
    "\n",
    "print(f\"Generando textos con {model_id_gen}...\")\n",
    "ood_texts = []\n",
    "for p in tqdm(prompts):\n",
    "    messages = [{\"role\": \"user\", \"content\": p}]\n",
    "    output = generator(messages, max_new_tokens=200, return_full_text=False, use_cache=False)\n",
    "    ood_texts.append(output[0]['generated_text'])\n",
    "\n",
    "# Evaluar Detección\n",
    "ood_preds = my_model.predict_proba(ood_texts)\n",
    "detection_rate_ood = np.mean(np.array(ood_preds) > 0.5) * 100\n",
    "\n",
    "print(f\"\\nResultados Generalización ({model_id_gen}):\")\n",
    "print(f\"Detection Rate (Recall): {detection_rate_ood:.2f}%\")\n",
    "if detection_rate_ood > 80:\n",
    "    print(\"✅ CONCLUSION: El modelo generaliza bien a arquitecturas nuevas.\")\n",
    "else:\n",
    "    print(\"⚠️ CONCLUSION: El modelo sufre con arquitecturas nuevas (Research Opportunity).\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Explicabilidad (SHAP Analysis)\n",
    "# Usamos SHAP sobre la parte tabular (LightGBM) de tu ensemble para entender qué características lingüísticas delatan a la IA.\n",
    "\n",
    "# %% [code]\n",
    "print(\"\\n--- Iniciando Análisis de Explicabilidad (SHAP) ---\")\n",
    "\n",
    "# 1. Preparamos datos para explicar\n",
    "# Usamos los textos OOD generados arriba como muestra de análisis\n",
    "X_explain = my_model.get_numeric_features(ood_texts)\n",
    "\n",
    "# 2. Explainer sobre el primer LightGBM del ensemble\n",
    "explainer = shap.TreeExplainer(my_model.lgb_models[0])\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "# 3. Visualización\n",
    "# Esta gráfica es ORO para el paper.\n",
    "plt.figure(figsize=(10, 6))\n",
    "# shap_values[1] suele ser la clase positiva (AI)\n",
    "if isinstance(shap_values, list):\n",
    "    vals = shap_values[1]\n",
    "else:\n",
    "    vals = shap_values\n",
    "\n",
    "shap.summary_plot(vals, X_explain, show=False)\n",
    "plt.title(\"Feature Importance (SHAP): ¿Qué delata a la IA?\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary.png\", dpi=300)\n",
    "print(\"Gráfica guardada como 'shap_summary.png'\")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Informe Final para el Paper\n",
    "# Generamos las filas LaTeX para tu tabla de resultados.\n",
    "\n",
    "# %% [code]\n",
    "print(\"\\n=== LaTeX Table Snippet ===\")\n",
    "print(f\"Method & Adversarial Acc & OOD Detection (Phi-3) \\\\\\\\\")\n",
    "print(f\"\\\\hline\")\n",
    "print(f\"Ours (Ensemble) & {100 - success_attack_rate:.2f}\\\\% & {detection_rate_ood:.2f}\\\\% \\\\\\\\\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
