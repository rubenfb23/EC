{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-intro",
   "metadata": {},
   "source": [
    "# HOPE: Hierarchical Optimization with Persistent Experience for AI-Generated Text Detection\n",
    "\n",
    "This notebook implements HOPE (Hierarchical Optimization with Persistent Experience), a neural architecture based on Google's Nested Learning paradigm for AI-generated text detection.\n",
    "\n",
    "## Key Components:\n",
    "1. **Neural Memory Module** - A learnable MLP that stores key-value associations with surprise-based updates\n",
    "2. **Continuum Memory System (CMS)** - Multiple memory modules updating at different frequencies\n",
    "3. **Nested Optimization** - Multi-level optimization with different update rates\n",
    "4. **Self-Referential Learning** - Memory modules that can optimize their own parameters\n",
    "\n",
    "## References:\n",
    "- [Nested Learning: A new ML paradigm for continual learning](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/)\n",
    "- [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)\n",
    "\n",
    "Flow: Setup → Data → Neural Memory Module → CMS → HOPE Model → Training → Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class HOPEConfig:\n",
    "    \"\"\"Configuration for HOPE model.\"\"\"\n",
    "    # Base encoder\n",
    "    base_model: str = \"microsoft/deberta-v3-base\"\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Neural Memory Module\n",
    "    memory_dim: int = 256  # Dimension of memory keys/values\n",
    "    memory_hidden_dim: int = 512  # Hidden dimension in memory MLP\n",
    "    memory_layers: int = 2  # Number of layers in memory MLP\n",
    "    \n",
    "    # Continuum Memory System (CMS)\n",
    "    num_memory_levels: int = 3  # Number of memory modules at different frequencies\n",
    "    update_frequencies: Tuple[int, ...] = (1, 4, 16)  # Update every N steps for each level\n",
    "    \n",
    "    # Surprise mechanism\n",
    "    surprise_momentum: float = 0.9  # eta_t: decay for past surprise\n",
    "    surprise_scale: float = 0.1  # theta_t: scale for momentary surprise\n",
    "    forget_rate: float = 0.01  # alpha_t: forgetting factor\n",
    "    \n",
    "    # Training\n",
    "    learning_rate: float = 2e-5\n",
    "    memory_lr: float = 1e-3  # Higher LR for memory modules (faster adaptation)\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 128\n",
    "    gradient_accumulation: int = 2\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # CV\n",
    "    n_splits: int = 5\n",
    "    \n",
    "    # Classifier\n",
    "    num_classes: int = 2\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Nested optimization\n",
    "    inner_steps: int = 1  # Steps for inner (memory) optimization per outer step\n",
    "    use_self_referential: bool = True  # Enable HOPE's self-referential updates\n",
    "\n",
    "config = HOPEConfig()\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths - using cleaned train/test datasets\n",
    "CWD = Path.cwd()\n",
    "\n",
    "# Find the data directory\n",
    "if (CWD / \"src/ai_vs_human\").exists():\n",
    "    DATA_DIR = CWD / \"src/ai_vs_human\"\n",
    "elif CWD.name == \"ai_vs_human\":\n",
    "    DATA_DIR = CWD\n",
    "else:\n",
    "    DATA_DIR = CWD\n",
    "\n",
    "# Specific dataset files\n",
    "TRAIN_FILE = \"merged_ai_human_multisocial_features_cleaned_train.csv\"\n",
    "TEST_FILE = \"merged_ai_human_multisocial_features_cleaned_test.csv\"\n",
    "\n",
    "DATA_PATH = DATA_DIR / TRAIN_FILE\n",
    "TEST_PATH = DATA_DIR / TEST_FILE\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Training data not found at {DATA_PATH}\")\n",
    "\n",
    "if not TEST_PATH.exists():\n",
    "    print(f\"Warning: Test data not found at {TEST_PATH}, will skip test inference\")\n",
    "    TEST_PATH = None\n",
    "\n",
    "WORK_DIR = DATA_DIR\n",
    "MODEL_DIR = WORK_DIR / \"models\" / \"hope\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OOF_DIR = WORK_DIR / \"oof\"\n",
    "OOF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Training data: {DATA_PATH}\")\n",
    "print(f\"Test data: {TEST_PATH}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-memory-intro",
   "metadata": {},
   "source": [
    "## Neural Memory Module\n",
    "\n",
    "The core of Titans/HOPE is a **neural memory module** - an MLP that learns key-value associations through surprise-based updates.\n",
    "\n",
    "### Key Equations:\n",
    "- **Memory Update**: $\\mathcal{M}_t = (1 - \\alpha_t)\\mathcal{M}_{t-1} + S_t$\n",
    "- **Surprise Metric**: $S_t = \\eta_t S_{t-1} - \\theta_t \\nabla\\ell(\\mathcal{M}_{t-1}; x_t)$\n",
    "- **Loss**: $\\ell(\\mathcal{M}_{t-1}; x_t) = \\|\\mathcal{M}_{t-1}(k_t) - v_t\\|_2^2$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_t$ controls forgetting\n",
    "- $\\eta_t$ is surprise momentum (past surprise decay)\n",
    "- $\\theta_t$ scales momentary surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-neural-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMemoryModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Memory Module from Titans architecture.\n",
    "    \n",
    "    Unlike traditional RNNs that use fixed-size vectors, this module uses\n",
    "    an MLP to store key-value associations in its parameters. The memory\n",
    "    is updated based on \"surprise\" - how unexpected each input is.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        memory_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 2,\n",
    "        surprise_momentum: float = 0.9,\n",
    "        surprise_scale: float = 0.1,\n",
    "        forget_rate: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.memory_dim = memory_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Hyperparameters for surprise-based updates\n",
    "        self.eta = surprise_momentum  # Past surprise decay\n",
    "        self.theta = surprise_scale   # Momentary surprise scale\n",
    "        self.alpha = forget_rate      # Forgetting factor\n",
    "        \n",
    "        # Key-Value projections\n",
    "        self.key_proj = nn.Linear(input_dim, memory_dim)\n",
    "        self.value_proj = nn.Linear(input_dim, memory_dim)\n",
    "        self.query_proj = nn.Linear(input_dim, memory_dim)\n",
    "        \n",
    "        # Memory MLP: maps keys to values\n",
    "        layers = []\n",
    "        dims = [memory_dim] + [hidden_dim] * (num_layers - 1) + [memory_dim]\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.SiLU())  # SiLU activation as in Titans\n",
    "                layers.append(nn.LayerNorm(dims[i + 1]))\n",
    "        self.memory_mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Learnable gates for adaptive surprise parameters\n",
    "        self.eta_gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.theta_gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.alpha_gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Accumulated surprise (momentum buffer)\n",
    "        self.register_buffer('surprise_momentum_buffer', None)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(memory_dim, input_dim)\n",
    "        \n",
    "    def compute_surprise(self, keys: torch.Tensor, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute surprise metric based on prediction error.\n",
    "        Surprise = gradient of associative memory loss w.r.t. input.\n",
    "        \"\"\"\n",
    "        # Forward pass through memory\n",
    "        predicted_values = self.memory_mlp(keys)\n",
    "        \n",
    "        # Associative memory loss: ||M(k) - v||^2\n",
    "        loss = F.mse_loss(predicted_values, values, reduction='none')\n",
    "        surprise = loss.mean(dim=-1, keepdim=True)  # Per-sample surprise\n",
    "        \n",
    "        return surprise, loss.mean()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        update_memory: bool = True,\n",
    "        return_surprise: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optional memory update.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, input_dim] or [batch, input_dim]\n",
    "            update_memory: Whether to update memory based on surprise\n",
    "            return_surprise: Whether to return surprise metrics\n",
    "        \"\"\"\n",
    "        # Handle 2D input\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to key-value-query space\n",
    "        keys = self.key_proj(x)  # [B, S, memory_dim]\n",
    "        values = self.value_proj(x)\n",
    "        queries = self.query_proj(x)\n",
    "        \n",
    "        # Normalize keys and queries (as in Titans)\n",
    "        keys = F.normalize(keys, p=2, dim=-1)\n",
    "        queries = F.normalize(queries, p=2, dim=-1)\n",
    "        \n",
    "        # Compute adaptive gate values\n",
    "        x_pooled = x.mean(dim=1)  # [B, input_dim]\n",
    "        eta_t = self.eta * self.eta_gate(x_pooled).squeeze(-1)  # Data-dependent decay\n",
    "        theta_t = self.theta * self.theta_gate(x_pooled).squeeze(-1)\n",
    "        alpha_t = self.alpha * self.alpha_gate(x_pooled).squeeze(-1)\n",
    "        \n",
    "        # Compute surprise\n",
    "        surprise, assoc_loss = self.compute_surprise(keys, values)\n",
    "        \n",
    "        # Memory read: retrieve based on query\n",
    "        retrieved = self.memory_mlp(queries)  # [B, S, memory_dim]\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_proj(retrieved)\n",
    "        \n",
    "        if output.size(1) == 1:\n",
    "            output = output.squeeze(1)\n",
    "        \n",
    "        if return_surprise:\n",
    "            return output, {\n",
    "                'surprise': surprise.mean(),\n",
    "                'assoc_loss': assoc_loss,\n",
    "                'eta': eta_t.mean(),\n",
    "                'theta': theta_t.mean(),\n",
    "                'alpha': alpha_t.mean(),\n",
    "            }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_memory_loss(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get the associative memory loss for training.\"\"\"\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        keys = F.normalize(self.key_proj(x), p=2, dim=-1)\n",
    "        values = self.value_proj(x)\n",
    "        \n",
    "        predicted = self.memory_mlp(keys)\n",
    "        return F.mse_loss(predicted, values)\n",
    "\n",
    "\n",
    "# Test the memory module\n",
    "print(\"Testing Neural Memory Module...\")\n",
    "mem = NeuralMemoryModule(input_dim=768, memory_dim=256, hidden_dim=512)\n",
    "test_input = torch.randn(4, 10, 768)  # [batch=4, seq=10, dim=768]\n",
    "output, metrics = mem(test_input, return_surprise=True)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-cms-intro",
   "metadata": {},
   "source": [
    "## Continuum Memory System (CMS)\n",
    "\n",
    "HOPE extends Titans with a **Continuum Memory System** - multiple memory modules that update at different frequencies:\n",
    "- **Fast memory** (level 0): Updates every step, captures immediate context\n",
    "- **Medium memory** (level 1): Updates every 4 steps, captures recent patterns  \n",
    "- **Slow memory** (level 2): Updates every 16 steps, consolidates abstract knowledge\n",
    "\n",
    "This creates a spectrum of memory timescales, similar to how human memory works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-cms",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuumMemorySystem(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuum Memory System (CMS) from HOPE.\n",
    "    \n",
    "    A stack of neural memory modules, each updating at a different frequency.\n",
    "    This creates a spectrum of memory timescales from fast (immediate context)\n",
    "    to slow (consolidated knowledge).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        memory_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_levels: int = 3,\n",
    "        update_frequencies: Tuple[int, ...] = (1, 4, 16),\n",
    "        surprise_momentum: float = 0.9,\n",
    "        surprise_scale: float = 0.1,\n",
    "        forget_rate: float = 0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.update_frequencies = update_frequencies[:num_levels]\n",
    "        \n",
    "        # Create memory modules for each level\n",
    "        # Slower levels have stronger momentum (more persistent memory)\n",
    "        self.memory_levels = nn.ModuleList([\n",
    "            NeuralMemoryModule(\n",
    "                input_dim=input_dim,\n",
    "                memory_dim=memory_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                surprise_momentum=surprise_momentum + 0.05 * i,  # Increase momentum for slower levels\n",
    "                surprise_scale=surprise_scale / (i + 1),  # Decrease scale for slower levels\n",
    "                forget_rate=forget_rate / (i + 1),  # Slower forgetting for slower levels\n",
    "            )\n",
    "            for i in range(num_levels)\n",
    "        ])\n",
    "        \n",
    "        # Fusion layer to combine outputs from all levels\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(input_dim * num_levels, input_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "        )\n",
    "        \n",
    "        # Learnable level weights\n",
    "        self.level_weights = nn.Parameter(torch.ones(num_levels) / num_levels)\n",
    "        \n",
    "        # Step counter for frequency-based updates\n",
    "        self.register_buffer('step_counter', torch.tensor(0))\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        return_level_outputs: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through all memory levels.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, input_dim] or [batch, input_dim]\n",
    "            return_level_outputs: Return individual level outputs for analysis\n",
    "        \"\"\"\n",
    "        level_outputs = []\n",
    "        level_metrics = []\n",
    "        \n",
    "        for i, (memory, freq) in enumerate(zip(self.memory_levels, self.update_frequencies)):\n",
    "            # Check if this level should update\n",
    "            should_update = (self.step_counter % freq == 0)\n",
    "            \n",
    "            # Forward through memory level\n",
    "            output, metrics = memory(x, update_memory=should_update, return_surprise=True)\n",
    "            level_outputs.append(output)\n",
    "            level_metrics.append(metrics)\n",
    "        \n",
    "        # Increment step counter\n",
    "        if self.training:\n",
    "            self.step_counter += 1\n",
    "        \n",
    "        # Weighted combination of level outputs\n",
    "        weights = F.softmax(self.level_weights, dim=0)\n",
    "        \n",
    "        # Handle different output shapes\n",
    "        if level_outputs[0].dim() == 2:\n",
    "            # [batch, dim] outputs\n",
    "            stacked = torch.stack(level_outputs, dim=-1)  # [B, D, L]\n",
    "            weighted = (stacked * weights.view(1, 1, -1)).sum(dim=-1)  # [B, D]\n",
    "            \n",
    "            # Also compute fusion output\n",
    "            concat = torch.cat(level_outputs, dim=-1)  # [B, D*L]\n",
    "            fused = self.fusion(concat)  # [B, D]\n",
    "            \n",
    "            # Combine weighted sum and fusion\n",
    "            output = weighted + fused\n",
    "        else:\n",
    "            # [batch, seq, dim] outputs\n",
    "            stacked = torch.stack(level_outputs, dim=-1)  # [B, S, D, L]\n",
    "            weighted = (stacked * weights.view(1, 1, 1, -1)).sum(dim=-1)  # [B, S, D]\n",
    "            \n",
    "            concat = torch.cat(level_outputs, dim=-1)  # [B, S, D*L]\n",
    "            fused = self.fusion(concat)  # [B, S, D]\n",
    "            \n",
    "            output = weighted + fused\n",
    "        \n",
    "        if return_level_outputs:\n",
    "            return output, level_outputs, level_metrics\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_total_memory_loss(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get combined associative memory loss from all levels.\"\"\"\n",
    "        total_loss = 0\n",
    "        for i, (memory, freq) in enumerate(zip(self.memory_levels, self.update_frequencies)):\n",
    "            # Weight loss by update frequency (faster levels contribute more)\n",
    "            weight = 1.0 / freq\n",
    "            total_loss += weight * memory.get_memory_loss(x)\n",
    "        return total_loss\n",
    "    \n",
    "    def reset_step_counter(self):\n",
    "        \"\"\"Reset step counter (call at start of each epoch).\"\"\"\n",
    "        self.step_counter.zero_()\n",
    "\n",
    "\n",
    "# Test CMS\n",
    "print(\"Testing Continuum Memory System...\")\n",
    "cms = ContinuumMemorySystem(\n",
    "    input_dim=768,\n",
    "    memory_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_levels=3,\n",
    "    update_frequencies=(1, 4, 16)\n",
    ")\n",
    "test_input = torch.randn(4, 768)\n",
    "output, level_outputs, metrics = cms(test_input, return_level_outputs=True)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Level weights: {F.softmax(cms.level_weights, dim=0).data}\")\n",
    "print(f\"Memory loss: {cms.get_total_memory_loss(test_input):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-hope-intro",
   "metadata": {},
   "source": [
    "## HOPE Model for Text Classification\n",
    "\n",
    "The full HOPE architecture combines:\n",
    "1. **Encoder** - DeBERTa-v3-base for text encoding\n",
    "2. **Continuum Memory System** - Multi-frequency memory for capturing patterns\n",
    "3. **Self-Referential Module** - Allows memory to optimize itself (HOPE's key innovation)\n",
    "4. **Classifier Head** - For binary AI/Human classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hope-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfReferentialModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Referential Module - HOPE's key innovation over Titans.\n",
    "    \n",
    "    This module allows the memory system to optimize its own parameters\n",
    "    through a meta-learning loop, enabling unbounded in-context learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Meta-network that predicts parameter updates\n",
    "        self.meta_net = nn.Sequential(\n",
    "            nn.Linear(dim * 2, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Tanh(),  # Bound updates\n",
    "        )\n",
    "        \n",
    "        # Gating for self-referential updates\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # Persistent memory (learnable, data-independent)\n",
    "        self.persistent_memory = nn.Parameter(torch.randn(1, dim) * 0.02)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, memory_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Self-referential forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Original input\n",
    "            memory_output: Output from CMS\n",
    "        \"\"\"\n",
    "        # Combine input and memory output for meta-learning\n",
    "        combined = torch.cat([x, memory_output], dim=-1)\n",
    "        \n",
    "        # Predict update direction\n",
    "        update = self.meta_net(combined)\n",
    "        \n",
    "        # Gated update\n",
    "        gate = self.gate(memory_output)\n",
    "        \n",
    "        # Apply self-referential update\n",
    "        output = memory_output + gate * update\n",
    "        \n",
    "        # Add persistent memory (expands to batch size)\n",
    "        persistent = self.persistent_memory.expand(x.size(0), -1)\n",
    "        output = output + 0.1 * persistent\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class HOPEClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    HOPE (Hierarchical Optimization with Persistent Experience) for Text Classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Text Encoder (DeBERTa) -> contextual embeddings\n",
    "    2. Continuum Memory System -> multi-frequency memory processing\n",
    "    3. Self-Referential Module -> meta-learning for memory optimization\n",
    "    4. Classifier Head -> AI/Human prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: HOPEConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Text encoder\n",
    "        self.encoder = AutoModel.from_pretrained(config.base_model)\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        \n",
    "        # Freeze early encoder layers (optional, for efficiency)\n",
    "        # for param in self.encoder.embeddings.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Continuum Memory System\n",
    "        self.cms = ContinuumMemorySystem(\n",
    "            input_dim=self.encoder_dim,\n",
    "            memory_dim=config.memory_dim,\n",
    "            hidden_dim=config.memory_hidden_dim,\n",
    "            num_levels=config.num_memory_levels,\n",
    "            update_frequencies=config.update_frequencies,\n",
    "            surprise_momentum=config.surprise_momentum,\n",
    "            surprise_scale=config.surprise_scale,\n",
    "            forget_rate=config.forget_rate,\n",
    "        )\n",
    "        \n",
    "        # Self-referential module (HOPE's addition)\n",
    "        self.self_ref = SelfReferentialModule(\n",
    "            dim=self.encoder_dim,\n",
    "            hidden_dim=config.memory_hidden_dim,\n",
    "        ) if config.use_self_referential else None\n",
    "        \n",
    "        # Pooling options\n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim, 1),\n",
    "        )\n",
    "        \n",
    "        # Classifier head\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder_dim * 2, self.encoder_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.LayerNorm(self.encoder_dim),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(self.encoder_dim, config.num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        return_memory_loss: bool = False,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs [batch, seq_len]\n",
    "            attention_mask: Attention mask [batch, seq_len]\n",
    "            labels: Optional labels for loss computation\n",
    "            return_memory_loss: Whether to return memory loss for nested optimization\n",
    "        \"\"\"\n",
    "        # Encode text\n",
    "        encoder_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        hidden_states = encoder_output.last_hidden_state  # [B, S, D]\n",
    "        \n",
    "        # Mean pooling for CMS input\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "        sum_hidden = (hidden_states * mask_expanded).sum(dim=1)\n",
    "        mean_pooled = sum_hidden / mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "        \n",
    "        # Attention-weighted pooling\n",
    "        attn_weights = self.attention_pool(hidden_states).squeeze(-1)  # [B, S]\n",
    "        attn_weights = attn_weights.masked_fill(~attention_mask.bool(), float('-inf'))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # [B, S]\n",
    "        attn_pooled = (hidden_states * attn_weights.unsqueeze(-1)).sum(dim=1)  # [B, D]\n",
    "        \n",
    "        # Process through Continuum Memory System\n",
    "        memory_output = self.cms(mean_pooled)  # [B, D]\n",
    "        \n",
    "        # Self-referential processing (HOPE's key feature)\n",
    "        if self.self_ref is not None:\n",
    "            memory_output = self.self_ref(mean_pooled, memory_output)\n",
    "        \n",
    "        # Combine encoder output with memory output\n",
    "        combined = torch.cat([attn_pooled, memory_output], dim=-1)  # [B, D*2]\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(combined)  # [B, num_classes]\n",
    "        \n",
    "        output = {'logits': logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            output['loss'] = loss\n",
    "        \n",
    "        if return_memory_loss:\n",
    "            # Memory loss for nested optimization\n",
    "            memory_loss = self.cms.get_total_memory_loss(mean_pooled)\n",
    "            output['memory_loss'] = memory_loss\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_memory_parameters(self):\n",
    "        \"\"\"Get parameters of memory modules (for separate optimizer).\"\"\"\n",
    "        params = list(self.cms.parameters())\n",
    "        if self.self_ref is not None:\n",
    "            params.extend(list(self.self_ref.parameters()))\n",
    "        return params\n",
    "    \n",
    "    def get_encoder_parameters(self):\n",
    "        \"\"\"Get parameters of encoder and classifier (for main optimizer).\"\"\"\n",
    "        params = list(self.encoder.parameters())\n",
    "        params.extend(list(self.classifier.parameters()))\n",
    "        params.extend(list(self.attention_pool.parameters()))\n",
    "        return params\n",
    "\n",
    "\n",
    "# Quick test\n",
    "print(\"Testing HOPE Classifier...\")\n",
    "test_config = HOPEConfig()\n",
    "# Smaller test model\n",
    "test_config.base_model = \"microsoft/deberta-v3-base\"\n",
    "print(f\"Loading {test_config.base_model}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for AI vs Human text classification.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        text_col: str = \"text\",\n",
    "        label_col: Optional[str] = \"label\",\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        text = str(self.df.loc[idx, self.text_col])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "        }\n",
    "        \n",
    "        if self.label_col and self.label_col in self.df.columns:\n",
    "            item['labels'] = torch.tensor(int(self.df.loc[idx, self.label_col]))\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(DATA_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH) if TEST_PATH is not None else None\n",
    "\n",
    "text_col = \"text\"\n",
    "label_col = \"label\"\n",
    "\n",
    "# Handle alternate column names\n",
    "if text_col not in train_df.columns:\n",
    "    for alt in [\"text_content\", \"content\"]:\n",
    "        if alt in train_df.columns:\n",
    "            train_df = train_df.rename(columns={alt: text_col})\n",
    "            if test_df is not None:\n",
    "                test_df = test_df.rename(columns={alt: text_col})\n",
    "            break\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Label distribution:\\n{train_df[label_col].value_counts()}\")\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"\\nTest shape: {test_df.shape}\")\n",
    "    if label_col in test_df.columns:\n",
    "        print(f\"Test label distribution:\\n{test_df[label_col].value_counts()}\")\n",
    "\n",
    "y = train_df[label_col].values\n",
    "\n",
    "# Setup CV\n",
    "N_SPLITS = config.n_splits\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "folds = list(cv.split(train_df, y))\n",
    "print(f\"\\nUsing {N_SPLITS}-fold StratifiedKFold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-training-intro",
   "metadata": {},
   "source": [
    "## Training with Nested Optimization\n",
    "\n",
    "HOPE uses **nested optimization** - two optimization loops running at different rates:\n",
    "\n",
    "1. **Outer Loop** (slow): Updates encoder and classifier parameters\n",
    "2. **Inner Loop** (fast): Updates memory module parameters\n",
    "\n",
    "This allows the memory to adapt quickly while the encoder learns stable representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(\n",
    "    fold: int,\n",
    "    train_idx: np.ndarray,\n",
    "    val_idx: np.ndarray,\n",
    "    config: HOPEConfig,\n",
    ") -> Tuple[np.ndarray, float, str]:\n",
    "    \"\"\"\n",
    "    Train a single fold with nested optimization.\n",
    "    \n",
    "    Returns:\n",
    "        oof_preds: Out-of-fold predictions for validation set\n",
    "        best_auc: Best validation AUC achieved\n",
    "        model_path: Path to saved model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Fold {fold + 1}/{N_SPLITS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data = train_df.iloc[train_idx]\n",
    "    val_data = train_df.iloc[val_idx]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model, use_fast=True)\n",
    "    \n",
    "    train_dataset = TextDataset(train_data, tokenizer, text_col, label_col, config.max_length)\n",
    "    val_dataset = TextDataset(val_data, tokenizer, text_col, label_col, config.max_length)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = HOPEClassifier(config).to(DEVICE)\n",
    "    \n",
    "    # Nested optimization: two optimizers with different learning rates\n",
    "    # Outer optimizer: encoder + classifier (slow)\n",
    "    outer_optimizer = AdamW(\n",
    "        model.get_encoder_parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "    \n",
    "    # Inner optimizer: memory modules (fast)\n",
    "    inner_optimizer = AdamW(\n",
    "        model.get_memory_parameters(),\n",
    "        lr=config.memory_lr,\n",
    "        weight_decay=config.weight_decay * 0.1,  # Less regularization for memory\n",
    "    )\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation\n",
    "    warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "    \n",
    "    outer_scheduler = CosineAnnealingLR(outer_optimizer, T_max=total_steps)\n",
    "    inner_scheduler = CosineAnnealingLR(inner_optimizer, T_max=total_steps)\n",
    "    \n",
    "    # Training loop\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    fold_dir = MODEL_DIR / f\"fold_{fold}\"\n",
    "    fold_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda') if DEVICE.type == 'cuda' else None\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        model.cms.reset_step_counter()  # Reset CMS step counter each epoch\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_memory_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        \n",
    "        outer_optimizer.zero_grad()\n",
    "        inner_optimizer.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(pbar):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast('cuda', enabled=scaler is not None):\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    return_memory_loss=True,\n",
    "                )\n",
    "                \n",
    "                # Classification loss\n",
    "                cls_loss = outputs['loss'] / config.gradient_accumulation\n",
    "                \n",
    "                # Memory loss (for nested optimization)\n",
    "                mem_loss = outputs['memory_loss'] / config.gradient_accumulation\n",
    "                \n",
    "                # Combined loss\n",
    "                total_loss = cls_loss + 0.1 * mem_loss  # Weight memory loss\n",
    "            \n",
    "            # Backward pass\n",
    "            if scaler is not None:\n",
    "                scaler.scale(total_loss).backward()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "            \n",
    "            epoch_loss += cls_loss.item() * config.gradient_accumulation\n",
    "            epoch_memory_loss += mem_loss.item() * config.gradient_accumulation\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (step + 1) % config.gradient_accumulation == 0:\n",
    "                if scaler is not None:\n",
    "                    # Clip gradients\n",
    "                    scaler.unscale_(outer_optimizer)\n",
    "                    scaler.unscale_(inner_optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    \n",
    "                    # Nested optimization: inner steps for memory\n",
    "                    for _ in range(config.inner_steps):\n",
    "                        scaler.step(inner_optimizer)\n",
    "                    \n",
    "                    # Outer step for encoder\n",
    "                    scaler.step(outer_optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    for _ in range(config.inner_steps):\n",
    "                        inner_optimizer.step()\n",
    "                    outer_optimizer.step()\n",
    "                \n",
    "                outer_optimizer.zero_grad()\n",
    "                inner_optimizer.zero_grad()\n",
    "                \n",
    "                outer_scheduler.step()\n",
    "                inner_scheduler.step()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{epoch_loss / num_batches:.4f}\",\n",
    "                'mem_loss': f\"{epoch_memory_loss / num_batches:.4f}\",\n",
    "            })\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=scaler is not None):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                probs = F.softmax(outputs['logits'], dim=-1)[:, 1]\n",
    "                val_preds.extend(probs.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].numpy())\n",
    "        \n",
    "        val_preds = np.array(val_preds)\n",
    "        val_labels = np.array(val_labels)\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_acc = accuracy_score(val_labels, (val_preds > 0.5).astype(int))\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Loss={epoch_loss/num_batches:.4f}, \"\n",
    "              f\"Val AUC={val_auc:.5f}, Val Acc={val_acc:.5f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  -> New best model! AUC: {best_auc:.5f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    model_path = fold_dir / \"best_model.pt\"\n",
    "    torch.save(best_model_state, model_path)\n",
    "    \n",
    "    # Load best model for final predictions\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get OOF predictions\n",
    "    oof_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=scaler is not None):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            probs = F.softmax(outputs['logits'], dim=-1)[:, 1]\n",
    "            oof_preds.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.array(oof_preds), best_auc, str(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for all folds\n",
    "oof_predictions = np.zeros(len(train_df))\n",
    "fold_aucs = []\n",
    "model_paths = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "    oof_preds, best_auc, model_path = train_fold(\n",
    "        fold=fold,\n",
    "        train_idx=train_idx,\n",
    "        val_idx=val_idx,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    oof_predictions[val_idx] = oof_preds\n",
    "    fold_aucs.append(best_auc)\n",
    "    model_paths.append(model_path)\n",
    "\n",
    "# Overall OOF performance\n",
    "overall_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"HOPE OOF Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Fold AUCs: {[f'{auc:.5f}' for auc in fold_aucs]}\")\n",
    "print(f\"Mean Fold AUC: {np.mean(fold_aucs):.5f} (+/- {np.std(fold_aucs):.5f})\")\n",
    "print(f\"Overall OOF AUC: {overall_auc:.5f}\")\n",
    "\n",
    "# Save OOF predictions\n",
    "pd.DataFrame({'oof_hope': oof_predictions}).to_csv(OOF_DIR / 'oof_hope.csv', index=False)\n",
    "print(f\"\\nSaved OOF predictions to {OOF_DIR / 'oof_hope.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set\n",
    "def predict_test(test_df: pd.DataFrame, model_paths: List[str], config: HOPEConfig) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate predictions on test set by averaging across all fold models.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model, use_fast=True)\n",
    "    test_dataset = TextDataset(test_df, tokenizer, text_col, None, config.max_length)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    all_preds = []\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        model = HOPEClassifier(config).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE, weights_only=True))\n",
    "        model.eval()\n",
    "        \n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=DEVICE.type == 'cuda'):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "                probs = F.softmax(outputs['logits'], dim=-1)[:, 1]\n",
    "                fold_preds.extend(probs.cpu().numpy())\n",
    "        \n",
    "        all_preds.append(fold_preds)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average across folds\n",
    "    return np.mean(all_preds, axis=0)\n",
    "\n",
    "\n",
    "if test_df is not None:\n",
    "    print(\"\\nRunning inference on test set...\")\n",
    "    test_preds = predict_test(test_df, model_paths, config)\n",
    "    \n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df.index if 'id' not in test_df.columns else test_df['id'],\n",
    "        'prediction': test_preds,\n",
    "    })\n",
    "    submission.to_csv(WORK_DIR / 'submission_hope.csv', index=False)\n",
    "    print(f\"Saved submission to {WORK_DIR / 'submission_hope.csv'}\")\n",
    "    \n",
    "    # If test has labels, compute AUC\n",
    "    if label_col in test_df.columns:\n",
    "        test_auc = roc_auc_score(test_df[label_col], test_preds)\n",
    "        test_acc = accuracy_score(test_df[label_col], (test_preds > 0.5).astype(int))\n",
    "        print(f\"Test AUC: {test_auc:.5f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.5f}\")\n",
    "else:\n",
    "    print(\"\\nNo test data available - skipping inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and comparison with other models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HOPE Model Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load OOF from other models for comparison\n",
    "comparison_models = {\n",
    "    'HOPE': oof_predictions,\n",
    "}\n",
    "\n",
    "for name in ['deberta', 'lgb', 'sgd', 'stack']:\n",
    "    path = OOF_DIR / f'oof_{name}.csv'\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        col = f'oof_{name}'\n",
    "        if col in df.columns and len(df) == len(y):\n",
    "            comparison_models[name.upper()] = df[col].values\n",
    "\n",
    "print(\"\\nOOF ROC-AUC Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "results = []\n",
    "for name, preds in comparison_models.items():\n",
    "    auc = roc_auc_score(y, preds)\n",
    "    results.append({'Model': name, 'OOF AUC': f'{auc:.5f}'})\n",
    "    print(f\"{name:15s}: {auc:.5f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HOPE Architecture Components:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  - Base Encoder: {config.base_model}\")\n",
    "print(f\"  - Memory Levels: {config.num_memory_levels}\")\n",
    "print(f\"  - Update Frequencies: {config.update_frequencies}\")\n",
    "print(f\"  - Self-Referential: {config.use_self_referential}\")\n",
    "print(f\"  - Nested Optimization: inner_steps={config.inner_steps}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-references",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Nested Learning: A new ML paradigm for continual learning](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/) - Google Research Blog\n",
    "2. [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663) - arXiv paper\n",
    "3. [HOPE: Hierarchical Optimization with Persistent Experience](https://www.etavrian.com/news/google-nested-learning-hope-outperforms-transformers) - NeurIPS 2025\n",
    "\n",
    "### Key Innovations:\n",
    "- **Neural Memory Module**: Uses MLP parameters to store key-value associations\n",
    "- **Surprise-Based Updates**: Memory updates proportional to prediction error (gradient)\n",
    "- **Continuum Memory System**: Multiple memory modules at different update frequencies\n",
    "- **Self-Referential Learning**: Memory can optimize its own parameters (HOPE's addition)\n",
    "- **Nested Optimization**: Separate learning rates for encoder vs memory modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
