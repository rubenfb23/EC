{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fb6256-fa84-41f2-82b6-ea7f04fd2e54",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "Whenever we have textual data, we need to apply several pre-processing steps to the data to transform words into numerical features that work with machine learning algorithms. The pre-processing steps for a problem depend mainly on the domain and the problem itself, hence, we don’t need to apply all steps to every problem. \n",
    "\n",
    "Here, we are going to learn how to apply common text preprocessing in Python. Apart from some very basic text processing, we will be using the [Natural Language Toolkit (NLTK)](https://www.nltk.org/).\n",
    "\n",
    "The case study is a dataset of news in Spanish, including the title and the body of the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c12ff4-7405-42e5-9152-4a54603f0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5665, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Noticia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un estudio impulsado por la Universidad de San...</td>\n",
       "      <td>El virus SARS-Cov-2 entró en España por la ciu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Las claves: qué es Montai y quién está detrás</td>\n",
       "      <td>¿Qué es Montai? ¿Qué relación guarda con las o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robots entregan domicilios en Medellín durante...</td>\n",
       "      <td>Unos 15 robots recorren las calles de Medellín...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grazón insiste en que un nuevo estado de alarm...</td>\n",
       "      <td>En una entrevista en Radio Euskadi, recogida p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox se sube a la ola de la extrema derecha eur...</td>\n",
       "      <td>\"España ha dejado de ser católica\", decía Manu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Titular  \\\n",
       "0  Un estudio impulsado por la Universidad de San...   \n",
       "1      Las claves: qué es Montai y quién está detrás   \n",
       "2  Robots entregan domicilios en Medellín durante...   \n",
       "3  Grazón insiste en que un nuevo estado de alarm...   \n",
       "4  Vox se sube a la ola de la extrema derecha eur...   \n",
       "\n",
       "                                             Noticia  \n",
       "0  El virus SARS-Cov-2 entró en España por la ciu...  \n",
       "1  ¿Qué es Montai? ¿Qué relación guarda con las o...  \n",
       "2  Unos 15 robots recorren las calles de Medellín...  \n",
       "3  En una entrevista en Radio Euskadi, recogida p...  \n",
       "4  \"España ha dejado de ser católica\", decía Manu...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re\n",
    "import pandas as pd \n",
    "\n",
    "with open('./data/noticias.txt') as json_file:\n",
    "    data = json.load(json_file)\n",
    "tuples = list(zip([noticia.get(\"titular\") for noticia in data],\n",
    "                  [noticia.get(\"texto\") for noticia in data]))\n",
    "df = pd.DataFrame(tuples, columns =['Titular', 'Noticia'])\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0a0ac-11fd-461e-9d63-3f1bd24784d3",
   "metadata": {},
   "source": [
    "## Special and undesired characters\n",
    "Let's start by performing a very basic clean up. \n",
    "In particular, we should account for special characters and other specificities that the texts may include \n",
    "and could generate noise when processing the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b719bb9c-841d-43a5-98da-ad53df2c109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Noticia</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un estudio impulsado por la Universidad de San...</td>\n",
       "      <td>El virus SARS-Cov-2 entró en España por la ciu...</td>\n",
       "      <td>el virus sars cov 2 entró en españa por la ciu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Las claves: qué es Montai y quién está detrás</td>\n",
       "      <td>¿Qué es Montai? ¿Qué relación guarda con las o...</td>\n",
       "      <td>qué es montai qué relación guarda con las otr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robots entregan domicilios en Medellín durante...</td>\n",
       "      <td>Unos 15 robots recorren las calles de Medellín...</td>\n",
       "      <td>unos 15 robots recorren las calles de medellín...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grazón insiste en que un nuevo estado de alarm...</td>\n",
       "      <td>En una entrevista en Radio Euskadi, recogida p...</td>\n",
       "      <td>en una entrevista en radio euskadi recogida po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox se sube a la ola de la extrema derecha eur...</td>\n",
       "      <td>\"España ha dejado de ser católica\", decía Manu...</td>\n",
       "      <td>españa ha dejado de ser católica decía manuel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Titular  \\\n",
       "0  Un estudio impulsado por la Universidad de San...   \n",
       "1      Las claves: qué es Montai y quién está detrás   \n",
       "2  Robots entregan domicilios en Medellín durante...   \n",
       "3  Grazón insiste en que un nuevo estado de alarm...   \n",
       "4  Vox se sube a la ola de la extrema derecha eur...   \n",
       "\n",
       "                                             Noticia  \\\n",
       "0  El virus SARS-Cov-2 entró en España por la ciu...   \n",
       "1  ¿Qué es Montai? ¿Qué relación guarda con las o...   \n",
       "2  Unos 15 robots recorren las calles de Medellín...   \n",
       "3  En una entrevista en Radio Euskadi, recogida p...   \n",
       "4  \"España ha dejado de ser católica\", decía Manu...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  el virus sars cov 2 entró en españa por la ciu...  \n",
       "1   qué es montai qué relación guarda con las otr...  \n",
       "2  unos 15 robots recorren las calles de medellín...  \n",
       "3  en una entrevista en radio euskadi recogida po...  \n",
       "4   españa ha dejado de ser católica decía manuel...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "    Common text cleaning steps.\n",
    "    \"\"\"\n",
    "    # Filter out special characters\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "    # Filter out very short words (here 1 char but often 2 char are also eliminated)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Replace consecutive spaces\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df[\"Tokens\"] = df.Noticia.apply(cleaning_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154d211-3e4e-403d-86d8-583cf9f486a9",
   "metadata": {},
   "source": [
    "## Basic text processing pipeline\n",
    "\n",
    "In general, texts are submitted to the following basic processing steps:\n",
    "<ul>\n",
    "    <li>Tokenization, i.e. split the text into sentences and the sentences into words. </li>\n",
    "    <li>Lowercase word conversion.</li>\n",
    "    <li>Stopword removal.</li>\n",
    "    <li>Too short word removal.</li>\n",
    "    <li>Stemming, i.e. words are reduced to their root form.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb14ea43-ace8-4bdd-87a1-fd8f2f5e65c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToktokTokenizer\n\u001b[32m      3\u001b[39m \u001b[33;03m''' There are multiple, good tokenizers available, this is just one of the most recent ones.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mThe tokenizer splits the text into sentences and the sentences into tokens, most of the times individual words (language dependent). \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mSo, it is relatively easy for you to see if the selected tokenizer is producing the desired output or not.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m'''\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "''' There are multiple, good tokenizers available, this is just one of the most recent ones.\n",
    "The tokenizer splits the text into sentences and the sentences into tokens, most of the times individual words (language dependent). \n",
    "So, it is relatively easy for you to see if the selected tokenizer is producing the desired output or not.\n",
    "'''\n",
    "\n",
    "tokenizer = ToktokTokenizer() \n",
    "df[\"Tokens\"] = df.Tokens.apply(tokenizer.tokenize)\n",
    "\n",
    "#df[\"Tokens\"] =word_tokenize(df.Tokens)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116c38f5-43bd-4660-8931-1ffaba9910e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Analia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Noticia</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un estudio impulsado por la Universidad de San...</td>\n",
       "      <td>El virus SARS-Cov-2 entró en España por la ciu...</td>\n",
       "      <td>[virus, sars, cov, entró, españa, ciudad, vito...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Las claves: qué es Montai y quién está detrás</td>\n",
       "      <td>¿Qué es Montai? ¿Qué relación guarda con las o...</td>\n",
       "      <td>[montai, relación, guarda, empresas, quién, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robots entregan domicilios en Medellín durante...</td>\n",
       "      <td>Unos 15 robots recorren las calles de Medellín...</td>\n",
       "      <td>[robots, recorren, calles, medellín, realizar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grazón insiste en que un nuevo estado de alarm...</td>\n",
       "      <td>En una entrevista en Radio Euskadi, recogida p...</td>\n",
       "      <td>[entrevista, radio, euskadi, recogida, europa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox se sube a la ola de la extrema derecha eur...</td>\n",
       "      <td>\"España ha dejado de ser católica\", decía Manu...</td>\n",
       "      <td>[españa, dejado, ser, católica, decía, manuel,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Titular  \\\n",
       "0  Un estudio impulsado por la Universidad de San...   \n",
       "1      Las claves: qué es Montai y quién está detrás   \n",
       "2  Robots entregan domicilios en Medellín durante...   \n",
       "3  Grazón insiste en que un nuevo estado de alarm...   \n",
       "4  Vox se sube a la ola de la extrema derecha eur...   \n",
       "\n",
       "                                             Noticia  \\\n",
       "0  El virus SARS-Cov-2 entró en España por la ciu...   \n",
       "1  ¿Qué es Montai? ¿Qué relación guarda con las o...   \n",
       "2  Unos 15 robots recorren las calles de Medellín...   \n",
       "3  En una entrevista en Radio Euskadi, recogida p...   \n",
       "4  \"España ha dejado de ser católica\", decía Manu...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [virus, sars, cov, entró, españa, ciudad, vito...  \n",
       "1  [montai, relación, guarda, empresas, quién, de...  \n",
       "2  [robots, recorren, calles, medellín, realizar,...  \n",
       "3  [entrevista, radio, euskadi, recogida, europa,...  \n",
       "4  [españa, dejado, ser, católica, decía, manuel,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Every language has stopwords, i.e. words that are used quite frequently for purposes \n",
    "# of syntactic constructiuon but that do not bear useful content. \n",
    "STOPWORDS = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "def eliminate_stopwords_digits(tokens):\n",
    "    \"\"\"\n",
    "    Eliminates stopwords and digits from the list of tokens.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in STOPWORDS \n",
    "            and not token.isdigit()]\n",
    "\n",
    "df[\"Tokens\"] = df.Tokens.apply(eliminate_stopwords_digits)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea0fc79-dffc-4088-a8b0-763dbfa2b1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titular</th>\n",
       "      <th>Noticia</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un estudio impulsado por la Universidad de San...</td>\n",
       "      <td>El virus SARS-Cov-2 entró en España por la ciu...</td>\n",
       "      <td>[virus, sars, cov, entro, españ, ciud, vitori,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Las claves: qué es Montai y quién está detrás</td>\n",
       "      <td>¿Qué es Montai? ¿Qué relación guarda con las o...</td>\n",
       "      <td>[montai, relacion, guard, empres, quien, detra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robots entregan domicilios en Medellín durante...</td>\n",
       "      <td>Unos 15 robots recorren las calles de Medellín...</td>\n",
       "      <td>[robots, recorr, call, medellin, realiz, entre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grazón insiste en que un nuevo estado de alarm...</td>\n",
       "      <td>En una entrevista en Radio Euskadi, recogida p...</td>\n",
       "      <td>[entrev, radi, euskadi, recog, europ, press, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vox se sube a la ola de la extrema derecha eur...</td>\n",
       "      <td>\"España ha dejado de ser católica\", decía Manu...</td>\n",
       "      <td>[españ, dej, ser, catol, dec, manuel, azañ, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Titular  \\\n",
       "0  Un estudio impulsado por la Universidad de San...   \n",
       "1      Las claves: qué es Montai y quién está detrás   \n",
       "2  Robots entregan domicilios en Medellín durante...   \n",
       "3  Grazón insiste en que un nuevo estado de alarm...   \n",
       "4  Vox se sube a la ola de la extrema derecha eur...   \n",
       "\n",
       "                                             Noticia  \\\n",
       "0  El virus SARS-Cov-2 entró en España por la ciu...   \n",
       "1  ¿Qué es Montai? ¿Qué relación guarda con las o...   \n",
       "2  Unos 15 robots recorren las calles de Medellín...   \n",
       "3  En una entrevista en Radio Euskadi, recogida p...   \n",
       "4  \"España ha dejado de ser católica\", decía Manu...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [virus, sars, cov, entro, españ, ciud, vitori,...  \n",
       "1  [montai, relacion, guard, empres, quien, detra...  \n",
       "2  [robots, recorr, call, medellin, realiz, entre...  \n",
       "3  [entrev, radi, euskadi, recog, europ, press, g...  \n",
       "4  [españ, dej, ser, catol, dec, manuel, azañ, co...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "'''\n",
    "Stemming is the process of reducing inflected words to their word stem, base or root form—generally a written word form.\n",
    "Again, there are multiple stemmers available. Snowball is one of the most commonly used. \n",
    "This process may take a while to complete...\n",
    "'''\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "def stem_palabras(tokens):\n",
    "    \"\"\"\n",
    "    Reduce cada palabra de una lista dada a su raíz.\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "df[\"Tokens\"] = df.Tokens.apply(stem_palabras)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ed6b4a-f7c8-4229-97cf-da5eacf0af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virus', 'sars', 'cov', 'entro', 'españ', 'ciud', 'vitori', 'torn', 'febrer', 'conclusion']\n"
     ]
    }
   ],
   "source": [
    "#Select a document to preview after preprocessing\n",
    "print(df.Tokens[0][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f86637-c3e8-4fc6-92ae-7f7cf50fc469",
   "metadata": {},
   "source": [
    "## Feature extraction: Bag of Words\n",
    "\n",
    "The cleaned text is not enough to be passed directly to the classification model. \n",
    "The features need to be numeric, not strings. \n",
    "\n",
    "There are many state-of-art approaches to extract features from the text data.\n",
    "The most simple and known method is the Bag-Of-Words (BOW) representation. \n",
    "It’s an algorithm that transforms the text into fixed-length vectors. \n",
    "This is possible by counting the number of times the word is present in a document. \n",
    "The word occurrences allow to compare different documents and evaluate their similarities for applications, \n",
    "such as search, document classification, and topic modeling.\n",
    "\n",
    "The reason for its name, “Bag-Of-Words”, is due to the fact that it represents the sentence as a bag of terms. \n",
    "It doesn’t take into account the order and the structure of the words, \n",
    "but it only checks if the words appear in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4984ce28-dd55-43ff-bcbd-43040eb2fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens: 47369\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "doc_term_matrix = Dictionary(df.Tokens)\n",
    "print(f'Número de tokens: {len(doc_term_matrix)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ecb62-8b29-4e79-9abe-f5f986f1b4bb",
   "metadata": {},
   "source": [
    "Filter out tokens that appear in less than 2 documents (absolute number) or \n",
    "more than 0.8 documents (fraction of total corpus size, not absolute number). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebf9510-2db9-482b-8778-73afdeb121c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens: 25516\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "no_below : absolute value\n",
    "no_above: percentual\n",
    "keep_n: number of tokens to be kept (i.e. the most frequent tokens)\n",
    "'''\n",
    "doc_term_matrix.filter_extremes(no_below=2, no_above = 0.8)\n",
    "print(f'Número de tokens: {len(doc_term_matrix)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd03626-7485-446e-9caa-6759b442c160",
   "metadata": {},
   "source": [
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘corpus’, then check one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f5a1ce-a4bf-4899-b02d-28af8491be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (25, 1), (26, 6), (29, 1), (40, 1), (41, 3), (44, 1), (48, 7), (52, 2), (67, 1), (68, 1), (77, 1), (86, 1), (94, 1), (96, 1), (108, 4), (116, 1), (118, 1), (121, 1), (131, 2), (146, 2), (149, 2), (164, 1), (172, 1), (176, 2), (178, 1), (193, 1), (204, 1), (210, 1), (222, 1), (235, 3), (236, 4), (238, 1), (245, 1), (268, 1), (276, 1), (283, 1), (295, 1), (299, 1), (311, 3), (312, 2), (339, 1), (349, 1), (367, 11), (372, 1), (394, 12), (407, 1), (413, 1), (431, 1), (436, 2), (439, 1), (440, 1), (450, 1), (454, 2), (462, 2), (475, 3), (478, 2), (492, 1), (498, 1), (502, 2), (513, 2), (525, 1), (531, 3), (549, 1), (561, 1), (574, 1), (587, 2), (615, 1), (631, 1), (640, 1), (650, 3), (653, 2), (656, 1), (660, 1), (677, 1), (680, 1), (683, 1), (684, 1), (686, 2), (694, 1), (732, 1), (784, 2), (793, 2), (794, 1), (804, 2), (817, 3), (830, 1), (839, 1), (840, 1), (842, 1), (852, 1), (866, 2), (913, 1), (915, 1), (917, 2), (921, 2), (933, 1), (958, 1), (1055, 1), (1057, 1), (1060, 1), (1073, 1), (1122, 1), (1140, 1), (1141, 1), (1142, 1), (1143, 1), (1144, 1), (1145, 1), (1146, 1), (1147, 1), (1148, 1), (1149, 1), (1150, 6), (1151, 1), (1152, 1), (1153, 1), (1154, 13), (1155, 1), (1156, 1), (1157, 1), (1158, 3), (1159, 1), (1160, 1), (1161, 1), (1162, 1), (1163, 1), (1164, 1), (1165, 1), (1166, 2), (1167, 3), (1168, 1), (1169, 1), (1170, 1), (1171, 1), (1172, 1), (1173, 1), (1174, 1), (1175, 1), (1176, 2), (1177, 2), (1178, 1), (1179, 1), (1180, 1), (1181, 1), (1182, 1), (1183, 1), (1184, 1), (1185, 1), (1186, 1), (1187, 1), (1188, 1), (1189, 1), (1190, 1), (1191, 1), (1192, 1), (1193, 1), (1194, 1), (1195, 1), (1196, 1), (1197, 2), (1198, 1), (1199, 1), (1200, 1), (1201, 1), (1202, 1), (1203, 2), (1204, 3), (1205, 1), (1206, 1), (1207, 1), (1208, 1), (1209, 1), (1210, 2), (1211, 1), (1212, 1), (1213, 1), (1214, 1), (1215, 10), (1216, 1), (1217, 1), (1218, 1), (1219, 1), (1220, 1), (1221, 1), (1222, 1), (1223, 1), (1224, 1), (1225, 1), (1226, 3), (1227, 1), (1228, 2), (1229, 1), (1230, 3)]\n",
      "afirm\n"
     ]
    }
   ],
   "source": [
    "# Corpus construction\n",
    "corpus = [doc_term_matrix.doc2bow(noticia) for noticia in df.Tokens]\n",
    "\n",
    "'''\n",
    "If we want to use tf_idf scores we should use the corresponding model...\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "'''\n",
    "\n",
    "# The BOW of one document \n",
    "# In the below example, the first five elements are [(3, 1), (25, 1), (26, 6), (29, 1), (40, 1) …].\n",
    "# Specifically, the tuple (3, 1) tells us that the word with id=3 shows one time. \n",
    "# You can know which token it is by printing the corresponding entry in the dictionary, i.e. doc_term_matrix[3].\n",
    "print(corpus[6])\n",
    "print(doc_term_matrix[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17660471-8131-41bc-91ba-507b96be3141",
   "metadata": {},
   "source": [
    "In the next classes, we will explore alternative feature extraction algorithms, namely the \n",
    "calculation of term frequency-inverse document frequency (TFIDF) scores. \n",
    "No algorithm outperforms the rest, the choice depends on the actual data as well as the objectives \n",
    "of the analysis. Often, it is interesting to compare model performance over different feature sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ic_env",
   "language": "python",
   "name": "ic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "properties": {
   "author": {
    "description": "Authors Name",
    "type": "string"
   },
   "email": {
    "description": "authors email, e.g. authors@email.com",
    "type": "string"
   },
   "institution": {
    "description": "the publishing institution",
    "items": {
     "type": "string"
    },
    "type": "array"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
